{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Copy of A6-k-means-with-text-data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O0Wv2htqEsD",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 6 - Implementing k-means with Text Data\n",
        "\n",
        "### Due: Tuesday, Aug 4th, 11:59 pm on Gradescope.\n",
        "\n",
        "This assignment will give you practice using the library `numpy` which is a very popular tool used in machine learning. You will implement the k-means algorithm in the specific setting of clustering text documents, but the algorithm is general to any setting. When properly executed, clustering uncovers valuable insights from a set of unlabeled documents.\n",
        "\n",
        "In this assignment, you will practice:\n",
        "\n",
        "* Cluster Wikipedia documents using k-means\n",
        "* Explore the role of random initialization on the quality of the clustering\n",
        "* Explore how results differ after changing the number of clusters\n",
        "* Evaluate clustering, both quantitatively and qualitatively\n",
        "\n",
        "Fill in the cells provided marked `TODO` with code to answer the questions. **Unless otherwise noted, every answer you submit should have code that clearly shows the answer in the output.** Answers submitted that do not have associated code that shows the answer may not be accepted for credit. \n",
        "\n",
        "**Make sure to restart the kernel and run all cells** (especially before turning it in) to make sure your code runs correctly. Answer the questions on Gradescope and make sure to download this file once you've finished the assignment and upload it to Canvas as well.\n",
        "\n",
        "> Copyright Â©2020 Vinitra Swamy, Valentina Staneva, Emily Fox and Hunter Schafer.  All rights reserved.  Permission is hereby granted to students registered for University of Washington CSE/STAT 416 for use solely during Summer Quarter 2020 for purposes of the course.  No other use, copying, distribution, or modification is permitted without prior written consent. Copyrights for third-party components of this work must be honored.  Instructors interested in reusing these course materials should contact the author.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdppLsfurIv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "\n",
        "def save_file(url, file_name):\n",
        "    r = requests.get(url)\n",
        "    with open(file_name, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    \n",
        "save_file('https://homes.cs.washington.edu/~vinitra/cse416/a6/people_wiki.csv',\n",
        "          'people_wiki.csv')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiEZ4OQ6qEsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSBcP9_aqEsI",
        "colab_type": "text"
      },
      "source": [
        "# Load data, Extract features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSpxKbGTqEsJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e1929f0e-4607-49d7-c18b-dc4eac42d4f1"
      },
      "source": [
        "wiki = pd.read_csv('people_wiki.csv')\n",
        "wiki = wiki.sample(frac=0.1, random_state=0) # Using 10% of the data as to reduce compute time\n",
        "wiki.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>URI</th>\n",
              "      <th>name</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50034</th>\n",
              "      <td>&lt;http://dbpedia.org/resource/Mauno_J%C3%A4rvel...</td>\n",
              "      <td>Mauno J%C3%A4rvel%C3%A4</td>\n",
              "      <td>mauno jrvel born 25 november 1949 in kaustinen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39362</th>\n",
              "      <td>&lt;http://dbpedia.org/resource/David_W._Jourdan&gt;</td>\n",
              "      <td>David W. Jourdan</td>\n",
              "      <td>david walter jourdan born december 5 1954 is a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20786</th>\n",
              "      <td>&lt;http://dbpedia.org/resource/Patrick_Roach&gt;</td>\n",
              "      <td>Patrick Roach</td>\n",
              "      <td>patrick roach born march 4 1969 is a canadian ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26367</th>\n",
              "      <td>&lt;http://dbpedia.org/resource/Louis_Sauer&gt;</td>\n",
              "      <td>Louis Sauer</td>\n",
              "      <td>louis lou sauer aka louis edward sauer born 19...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14855</th>\n",
              "      <td>&lt;http://dbpedia.org/resource/Marty_Keough&gt;</td>\n",
              "      <td>Marty Keough</td>\n",
              "      <td>richard martin keough born april 14 1934 in oa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     URI  ...                                               text\n",
              "50034  <http://dbpedia.org/resource/Mauno_J%C3%A4rvel...  ...  mauno jrvel born 25 november 1949 in kaustinen...\n",
              "39362     <http://dbpedia.org/resource/David_W._Jourdan>  ...  david walter jourdan born december 5 1954 is a...\n",
              "20786        <http://dbpedia.org/resource/Patrick_Roach>  ...  patrick roach born march 4 1969 is a canadian ...\n",
              "26367          <http://dbpedia.org/resource/Louis_Sauer>  ...  louis lou sauer aka louis edward sauer born 19...\n",
              "14855         <http://dbpedia.org/resource/Marty_Keough>  ...  richard martin keough born april 14 1934 in oa...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gsmRKsqEsN",
        "colab_type": "text"
      },
      "source": [
        "To work with text data, we must first convert the documents into numerical features. Like Assignment 3, let's extract TF-IDF features for each article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5mxiLcJqEsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_df=0.95)  # ignore words with very high doc frequency\n",
        "tf_idf = vectorizer.fit_transform(wiki['text'])\n",
        "words = vectorizer.get_feature_names()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTBQNXb1H-Mq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f62e0841-11f4-4fc5-960e-657a934accf9"
      },
      "source": [
        "words"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['00',\n",
              " '000',\n",
              " '0001',\n",
              " '00014338',\n",
              " '0005sec',\n",
              " '0007207328',\n",
              " '0007213506',\n",
              " '000721426xhe',\n",
              " '001',\n",
              " '003',\n",
              " '004',\n",
              " '005',\n",
              " '0072131772',\n",
              " '0072131896',\n",
              " '0072222611',\n",
              " '0072225351',\n",
              " '0080',\n",
              " '009',\n",
              " '01',\n",
              " '010',\n",
              " '011',\n",
              " '012',\n",
              " '014',\n",
              " '015',\n",
              " '018',\n",
              " '01analyzing',\n",
              " '01he',\n",
              " '02',\n",
              " '0203',\n",
              " '02032010',\n",
              " '023',\n",
              " '024',\n",
              " '0273langton',\n",
              " '029',\n",
              " '03',\n",
              " '030',\n",
              " '03082008',\n",
              " '0312913451',\n",
              " '0387',\n",
              " '03his',\n",
              " '04',\n",
              " '040',\n",
              " '0405',\n",
              " '0443073457',\n",
              " '05',\n",
              " '050',\n",
              " '051',\n",
              " '055',\n",
              " '06',\n",
              " '060',\n",
              " '0632037571',\n",
              " '067',\n",
              " '0679400036',\n",
              " '06in',\n",
              " '07',\n",
              " '070',\n",
              " '0708',\n",
              " '071',\n",
              " '0710',\n",
              " '0714542954',\n",
              " '0743260481',\n",
              " '075',\n",
              " '0773507450bakan',\n",
              " '0773522522',\n",
              " '07in',\n",
              " '08',\n",
              " '0801862701',\n",
              " '0802075959',\n",
              " '0802111068',\n",
              " '0803747527',\n",
              " '080m',\n",
              " '08142013',\n",
              " '0879101733',\n",
              " '088',\n",
              " '0889202206',\n",
              " '0897501446',\n",
              " '09',\n",
              " '09008050',\n",
              " '0908630360',\n",
              " '091',\n",
              " '0944803733ted',\n",
              " '096',\n",
              " '0961862203kathy',\n",
              " '097',\n",
              " '10',\n",
              " '100',\n",
              " '1000',\n",
              " '10000',\n",
              " '100000',\n",
              " '1000000',\n",
              " '100000pappas',\n",
              " '100001996',\n",
              " '10000jata',\n",
              " '10000m',\n",
              " '10000metre',\n",
              " '1000m',\n",
              " '1000page',\n",
              " '1000plus',\n",
              " '1000pm',\n",
              " '1000th',\n",
              " '1000yard',\n",
              " '1001',\n",
              " '100106',\n",
              " '1002',\n",
              " '100200',\n",
              " '1003',\n",
              " '10030they',\n",
              " '1005',\n",
              " '1007',\n",
              " '1009',\n",
              " '100defever',\n",
              " '100design',\n",
              " '100garland',\n",
              " '100girls',\n",
              " '100he',\n",
              " '100inch',\n",
              " '100k',\n",
              " '100lingmerth',\n",
              " '100lovato',\n",
              " '100m',\n",
              " '100mgilbert',\n",
              " '100reception',\n",
              " '100strong',\n",
              " '100t',\n",
              " '100th',\n",
              " '100yard',\n",
              " '100yearold',\n",
              " '101',\n",
              " '1010',\n",
              " '10106',\n",
              " '1011',\n",
              " '1012',\n",
              " '1013',\n",
              " '1014',\n",
              " '1014000',\n",
              " '10141',\n",
              " '1015',\n",
              " '10153',\n",
              " '101688',\n",
              " '1018',\n",
              " '1019',\n",
              " '10194',\n",
              " '101st',\n",
              " '102',\n",
              " '1020',\n",
              " '1021',\n",
              " '1023',\n",
              " '1023robinson',\n",
              " '1024',\n",
              " '1024bit',\n",
              " '1026',\n",
              " '1027',\n",
              " '10271059',\n",
              " '1028',\n",
              " '102s',\n",
              " '103',\n",
              " '1030',\n",
              " '1030pm',\n",
              " '103106liu',\n",
              " '1031snow',\n",
              " '1033',\n",
              " '10341',\n",
              " '1035',\n",
              " '10352',\n",
              " '10353',\n",
              " '1037',\n",
              " '10371084',\n",
              " '1038',\n",
              " '10383',\n",
              " '10385',\n",
              " '103rd',\n",
              " '103vote',\n",
              " '104',\n",
              " '10428',\n",
              " '1043',\n",
              " '10430',\n",
              " '1045',\n",
              " '10450',\n",
              " '1049',\n",
              " '104913',\n",
              " '104th',\n",
              " '105',\n",
              " '10501',\n",
              " '10503',\n",
              " '1053',\n",
              " '105335',\n",
              " '105362',\n",
              " '10538',\n",
              " '1054',\n",
              " '105525',\n",
              " '1056',\n",
              " '1057ondayko',\n",
              " '105th',\n",
              " '106',\n",
              " '1060',\n",
              " '10600',\n",
              " '106382tann',\n",
              " '1065bnhe',\n",
              " '1069',\n",
              " '106pschuster',\n",
              " '107',\n",
              " '1070',\n",
              " '10711',\n",
              " '10716',\n",
              " '10726',\n",
              " '10736',\n",
              " '1075',\n",
              " '1076',\n",
              " '1079',\n",
              " '107th',\n",
              " '108',\n",
              " '10803',\n",
              " '10804',\n",
              " '1082',\n",
              " '10825',\n",
              " '1083',\n",
              " '108906',\n",
              " '108th',\n",
              " '109',\n",
              " '1090',\n",
              " '10903',\n",
              " '10904',\n",
              " '10908',\n",
              " '10938',\n",
              " '1096',\n",
              " '1097',\n",
              " '10976',\n",
              " '109800acampora',\n",
              " '1099',\n",
              " '109th',\n",
              " '10a4',\n",
              " '10am',\n",
              " '10am12',\n",
              " '10am12pm',\n",
              " '10foot',\n",
              " '10his',\n",
              " '10k',\n",
              " '10kirchners',\n",
              " '10m',\n",
              " '10million',\n",
              " '10monthcourse',\n",
              " '10news',\n",
              " '10part',\n",
              " '10round',\n",
              " '10s',\n",
              " '10seeded',\n",
              " '10songs',\n",
              " '10th',\n",
              " '10this',\n",
              " '10underpar',\n",
              " '10win',\n",
              " '10year',\n",
              " '10yearold',\n",
              " '10years',\n",
              " '11',\n",
              " '110',\n",
              " '1100',\n",
              " '11000',\n",
              " '110000',\n",
              " '1100th',\n",
              " '1101',\n",
              " '1104',\n",
              " '1105',\n",
              " '1106',\n",
              " '1107',\n",
              " '11072',\n",
              " '1108',\n",
              " '11081967',\n",
              " '110mhh',\n",
              " '110reception',\n",
              " '110th',\n",
              " '111',\n",
              " '1110',\n",
              " '1111',\n",
              " '111117',\n",
              " '1112',\n",
              " '1113',\n",
              " '1114',\n",
              " '1116',\n",
              " '1116in',\n",
              " '1118',\n",
              " '1118year',\n",
              " '111th',\n",
              " '112',\n",
              " '112009',\n",
              " '1121',\n",
              " '1124012008',\n",
              " '1125',\n",
              " '1126',\n",
              " '1129',\n",
              " '112th',\n",
              " '113',\n",
              " '1134',\n",
              " '1137',\n",
              " '11371170',\n",
              " '11378',\n",
              " '1138',\n",
              " '11382',\n",
              " '113pm',\n",
              " '113th',\n",
              " '113thwatkinson',\n",
              " '113to55',\n",
              " '114',\n",
              " '1143',\n",
              " '1145',\n",
              " '1146',\n",
              " '114th',\n",
              " '115',\n",
              " '11500',\n",
              " '1152',\n",
              " '11541',\n",
              " '11571225',\n",
              " '1158',\n",
              " '115a',\n",
              " '115th',\n",
              " '116',\n",
              " '1162',\n",
              " '1166',\n",
              " '11691',\n",
              " '116th',\n",
              " '117',\n",
              " '11700',\n",
              " '1171',\n",
              " '11723',\n",
              " '1173',\n",
              " '118',\n",
              " '1184',\n",
              " '11844',\n",
              " '1186',\n",
              " '1187',\n",
              " '118in',\n",
              " '118th',\n",
              " '118use',\n",
              " '119',\n",
              " '1195',\n",
              " '11988126',\n",
              " '1199',\n",
              " '119th',\n",
              " '11am3pm',\n",
              " '11b',\n",
              " '11game',\n",
              " '11h3',\n",
              " '11in',\n",
              " '11m',\n",
              " '11member',\n",
              " '11pm',\n",
              " '11season',\n",
              " '11seat',\n",
              " '11th',\n",
              " '11thwallace',\n",
              " '11twelve',\n",
              " '11year',\n",
              " '11yes',\n",
              " '12',\n",
              " '120',\n",
              " '1200',\n",
              " '12000',\n",
              " '120000',\n",
              " '1200page',\n",
              " '1200squaremile',\n",
              " '1201',\n",
              " '12021974',\n",
              " '1205',\n",
              " '12091215',\n",
              " '12095',\n",
              " '120m',\n",
              " '120pmueller',\n",
              " '120th',\n",
              " '120yearold',\n",
              " '121',\n",
              " '1210',\n",
              " '1211',\n",
              " '12131',\n",
              " '1218',\n",
              " '1219',\n",
              " '121st',\n",
              " '122',\n",
              " '12201',\n",
              " '1221',\n",
              " '122238',\n",
              " '12260',\n",
              " '123',\n",
              " '1232',\n",
              " '1233',\n",
              " '1234',\n",
              " '1237',\n",
              " '123pm',\n",
              " '124',\n",
              " '12436',\n",
              " '12451',\n",
              " '124655',\n",
              " '124773',\n",
              " '1249',\n",
              " '124squaremile',\n",
              " '125',\n",
              " '1250',\n",
              " '12500',\n",
              " '125000',\n",
              " '1252',\n",
              " '1255',\n",
              " '1258',\n",
              " '1259',\n",
              " '125cc',\n",
              " '125pound',\n",
              " '125th',\n",
              " '126',\n",
              " '1260',\n",
              " '1264',\n",
              " '12642',\n",
              " '1269',\n",
              " '127',\n",
              " '12711',\n",
              " '127227',\n",
              " '1273',\n",
              " '1274',\n",
              " '127563',\n",
              " '1276',\n",
              " '1279',\n",
              " '128',\n",
              " '12813',\n",
              " '1282',\n",
              " '1284',\n",
              " '129',\n",
              " '1293',\n",
              " '1295th',\n",
              " '1297',\n",
              " '1299',\n",
              " '129million',\n",
              " '12a',\n",
              " '12as',\n",
              " '12club',\n",
              " '12f4',\n",
              " '12foot',\n",
              " '12footwide',\n",
              " '12he',\n",
              " '12hour',\n",
              " '12i1927',\n",
              " '12inch',\n",
              " '12kreuters',\n",
              " '12metre',\n",
              " '12minute',\n",
              " '12month',\n",
              " '12page',\n",
              " '12part',\n",
              " '12salinas',\n",
              " '12song',\n",
              " '12string',\n",
              " '12th',\n",
              " '12thin',\n",
              " '12year',\n",
              " '13',\n",
              " '130',\n",
              " '1300',\n",
              " '13000',\n",
              " '130000',\n",
              " '1300000tann',\n",
              " '1300th',\n",
              " '130228',\n",
              " '1303',\n",
              " '1305',\n",
              " '1308',\n",
              " '131',\n",
              " '1310',\n",
              " '1311',\n",
              " '1312',\n",
              " '1314',\n",
              " '1318',\n",
              " '131pmueller',\n",
              " '131st',\n",
              " '132',\n",
              " '1322',\n",
              " '1322commentary',\n",
              " '1324',\n",
              " '132410he',\n",
              " '1325',\n",
              " '133',\n",
              " '1330',\n",
              " '133228',\n",
              " '1334',\n",
              " '1336',\n",
              " '133rd',\n",
              " '134',\n",
              " '1340',\n",
              " '1342it',\n",
              " '1343',\n",
              " '1345',\n",
              " '134620',\n",
              " '135',\n",
              " '1350',\n",
              " '135000',\n",
              " '1354',\n",
              " '13558',\n",
              " '1356',\n",
              " '135613',\n",
              " '1359',\n",
              " '135th',\n",
              " '136',\n",
              " '1360',\n",
              " '13604',\n",
              " '13612',\n",
              " '13634',\n",
              " '1365',\n",
              " '136pmueller',\n",
              " '137',\n",
              " '137149',\n",
              " '1372223',\n",
              " '1373',\n",
              " '1376',\n",
              " '137a',\n",
              " '137th',\n",
              " '138',\n",
              " '1383',\n",
              " '1384',\n",
              " '1385',\n",
              " '13857',\n",
              " '138916',\n",
              " '139',\n",
              " '1393940',\n",
              " '1395these',\n",
              " '1399th',\n",
              " '13benvenisti',\n",
              " '13episode',\n",
              " '13for20',\n",
              " '13for75',\n",
              " '13game',\n",
              " '13ng3',\n",
              " '13part',\n",
              " '13piece',\n",
              " '13px',\n",
              " '13season',\n",
              " '13th',\n",
              " '13th18th',\n",
              " '13thcentury',\n",
              " '13tsheri',\n",
              " '13volume',\n",
              " '13week',\n",
              " '13year',\n",
              " '13yearold',\n",
              " '14',\n",
              " '140',\n",
              " '1400',\n",
              " '14000',\n",
              " '1400000in',\n",
              " '1401417',\n",
              " '14027',\n",
              " '141',\n",
              " '1411',\n",
              " '14123',\n",
              " '1413',\n",
              " '1414',\n",
              " '1415',\n",
              " '1416',\n",
              " '14170',\n",
              " '141873as',\n",
              " '141st',\n",
              " '142',\n",
              " '142124',\n",
              " '1422',\n",
              " '1422011',\n",
              " '1425',\n",
              " '1425992021',\n",
              " '143',\n",
              " '1430',\n",
              " '1431',\n",
              " '143159the',\n",
              " '143rd',\n",
              " '144',\n",
              " '1440',\n",
              " '14401496',\n",
              " '144159',\n",
              " '1443',\n",
              " '1444',\n",
              " '1447',\n",
              " '14497336',\n",
              " '144th',\n",
              " '145',\n",
              " '145000',\n",
              " '1457',\n",
              " '145for630',\n",
              " '145th',\n",
              " '146',\n",
              " '14685',\n",
              " '147',\n",
              " '1473',\n",
              " '1475',\n",
              " '148',\n",
              " '148000',\n",
              " '1481',\n",
              " '148100he',\n",
              " '14826azrad',\n",
              " '1484',\n",
              " '1485',\n",
              " '1487',\n",
              " '149',\n",
              " '1491',\n",
              " '1494',\n",
              " '149pmueller',\n",
              " '14am',\n",
              " '14andahalf',\n",
              " '14andr',\n",
              " '14andunder',\n",
              " '14as',\n",
              " '14bc2',\n",
              " '14foot',\n",
              " '14footwide',\n",
              " '14for',\n",
              " '14francisco',\n",
              " '14game',\n",
              " '14he',\n",
              " '14in',\n",
              " '14inning',\n",
              " '14naroditsky',\n",
              " '14no',\n",
              " '14on',\n",
              " '14pm',\n",
              " '14th',\n",
              " '14thcentury',\n",
              " '14thin',\n",
              " '14time',\n",
              " '14toneperoctave',\n",
              " '14underpar',\n",
              " '14year',\n",
              " '14yearold',\n",
              " '14yearolds',\n",
              " '15',\n",
              " '150',\n",
              " '1500',\n",
              " '15000',\n",
              " '150000',\n",
              " '1500m',\n",
              " '1502',\n",
              " '1505948galndez',\n",
              " '150th',\n",
              " '150up',\n",
              " '151',\n",
              " '1511',\n",
              " '15114',\n",
              " '1513',\n",
              " '1515',\n",
              " '1517',\n",
              " '152',\n",
              " '15230901in',\n",
              " '15251534',\n",
              " '1526',\n",
              " '1528',\n",
              " '153',\n",
              " '153146',\n",
              " '153159',\n",
              " '15361',\n",
              " '1536273000m',\n",
              " '15390',\n",
              " '153rd',\n",
              " '154',\n",
              " '1542',\n",
              " '1543',\n",
              " '155',\n",
              " '155173in',\n",
              " '15521610',\n",
              " '156',\n",
              " '15600000',\n",
              " '1561',\n",
              " '1563',\n",
              " '1565',\n",
              " '1567',\n",
              " '15671629',\n",
              " '157',\n",
              " '1570',\n",
              " '1571',\n",
              " '157137',\n",
              " '157337',\n",
              " '15754at',\n",
              " '158',\n",
              " '1581603266',\n",
              " '1585',\n",
              " '159',\n",
              " '1590591518',\n",
              " '1593',\n",
              " '15934',\n",
              " '159in',\n",
              " '15bn',\n",
              " '15bukals',\n",
              " '15hourlong',\n",
              " '15k',\n",
              " '15m',\n",
              " '15part',\n",
              " '15plus',\n",
              " '15qf3',\n",
              " '15s',\n",
              " '15taheri',\n",
              " '15th',\n",
              " '15thcentury',\n",
              " '15the',\n",
              " '15time',\n",
              " '15year',\n",
              " '15yearold',\n",
              " '15years',\n",
              " '16',\n",
              " '160',\n",
              " '1600',\n",
              " '16000',\n",
              " '160000',\n",
              " '1600meter',\n",
              " '1600mile',\n",
              " '16031707',\n",
              " '16061960ganbari',\n",
              " '160th',\n",
              " '161',\n",
              " '1610',\n",
              " '1611652002',\n",
              " '1612',\n",
              " '16123',\n",
              " '1613',\n",
              " '1614matti',\n",
              " '1616',\n",
              " '161821',\n",
              " '161989',\n",
              " '162',\n",
              " '16200',\n",
              " '1621',\n",
              " '1622',\n",
              " '1624',\n",
              " '1626',\n",
              " '1629',\n",
              " '162nd',\n",
              " '163',\n",
              " '1630s',\n",
              " '1632',\n",
              " '163288306',\n",
              " '1636',\n",
              " '163628',\n",
              " '16369',\n",
              " '163700',\n",
              " '1637harris',\n",
              " '1638',\n",
              " '164',\n",
              " '16431652',\n",
              " '16431715',\n",
              " '1644',\n",
              " '16487',\n",
              " '165',\n",
              " '1655',\n",
              " '1658',\n",
              " '165foot',\n",
              " '166',\n",
              " '166murray',\n",
              " '166th',\n",
              " '167',\n",
              " '1670',\n",
              " '1670s',\n",
              " '1679',\n",
              " '167th',\n",
              " '168',\n",
              " '1680',\n",
              " '168000',\n",
              " '1684',\n",
              " '16875',\n",
              " '1689',\n",
              " '168lb',\n",
              " '169',\n",
              " '1690s',\n",
              " '16913',\n",
              " '1692',\n",
              " '1694',\n",
              " '1697',\n",
              " '16acre',\n",
              " '16be3',\n",
              " '16campus',\n",
              " '16cerljen',\n",
              " '16digiamarino',\n",
              " '16foot',\n",
              " '16g',\n",
              " '16game',\n",
              " '16gelmetti',\n",
              " '16james',\n",
              " '16jonny',\n",
              " '16member',\n",
              " '16mm',\n",
              " '16point',\n",
              " '16possession',\n",
              " '16s',\n",
              " '16she',\n",
              " '16th',\n",
              " '16thcentury',\n",
              " '16these',\n",
              " '16time',\n",
              " '16year',\n",
              " '16yearold',\n",
              " '17',\n",
              " '170',\n",
              " '1700',\n",
              " '17000',\n",
              " '17011754',\n",
              " '1702',\n",
              " '17023',\n",
              " '1703',\n",
              " '1706000',\n",
              " '17096',\n",
              " '171',\n",
              " '1711',\n",
              " '17111',\n",
              " '1712',\n",
              " '1714',\n",
              " '1716',\n",
              " '1718',\n",
              " '1719',\n",
              " '171981',\n",
              " '171sthighest',\n",
              " '172',\n",
              " '17210',\n",
              " '1723',\n",
              " '1725',\n",
              " '1726',\n",
              " '1727',\n",
              " '1728',\n",
              " '1728des',\n",
              " '1729',\n",
              " '172nd',\n",
              " '173',\n",
              " '173050',\n",
              " '17338',\n",
              " '173pmueller',\n",
              " '174',\n",
              " '17405',\n",
              " '1744',\n",
              " '175',\n",
              " '17500',\n",
              " '17581',\n",
              " '175cmm',\n",
              " '175m',\n",
              " '175min',\n",
              " '176',\n",
              " '1760',\n",
              " '176000',\n",
              " '17601850',\n",
              " '1767major',\n",
              " '17690',\n",
              " '176m',\n",
              " '177',\n",
              " '1776',\n",
              " '1776thicker',\n",
              " '1777',\n",
              " '1778',\n",
              " '1779',\n",
              " '178',\n",
              " '1780',\n",
              " '17811820',\n",
              " '17831839',\n",
              " '1784',\n",
              " '17891850',\n",
              " '17891989',\n",
              " '179',\n",
              " '17911840',\n",
              " '1792',\n",
              " '1796',\n",
              " '1797',\n",
              " '17axb5',\n",
              " '17b',\n",
              " '17castle',\n",
              " '17city',\n",
              " '17game',\n",
              " '17he',\n",
              " '17minute',\n",
              " '17th',\n",
              " '17thcentury',\n",
              " '17x',\n",
              " '17year',\n",
              " '17yearold',\n",
              " '18',\n",
              " '180',\n",
              " '1800',\n",
              " '18000',\n",
              " '180000',\n",
              " '18001990',\n",
              " '1800having',\n",
              " '1801',\n",
              " '1802',\n",
              " '1803',\n",
              " '1805',\n",
              " '18061848',\n",
              " '18079',\n",
              " '180pound',\n",
              " '181',\n",
              " '1810',\n",
              " '1811',\n",
              " '1813',\n",
              " '1814',\n",
              " '18152007',\n",
              " '182',\n",
              " '1820',\n",
              " '182000',\n",
              " '1821',\n",
              " '18219',\n",
              " '1822032000',\n",
              " '1825',\n",
              " '18276',\n",
              " '18291903',\n",
              " '182nd',\n",
              " '183',\n",
              " '183000as',\n",
              " '18301980',\n",
              " '1830in',\n",
              " '1834',\n",
              " '18341902',\n",
              " '18344208',\n",
              " '18347797',\n",
              " '1835',\n",
              " '18351231',\n",
              " '18351910kvrne',\n",
              " '1837',\n",
              " '18381914',\n",
              " '183rdthis',\n",
              " '184',\n",
              " '1840',\n",
              " '1842',\n",
              " '18431912',\n",
              " '1848',\n",
              " '18481849',\n",
              " '18481918',\n",
              " '1849',\n",
              " '18492',\n",
              " '185',\n",
              " '1850',\n",
              " '185000',\n",
              " '1850000',\n",
              " '1852',\n",
              " '1854',\n",
              " '1855',\n",
              " '18571947',\n",
              " '1858',\n",
              " '1859',\n",
              " '185cms',\n",
              " '186',\n",
              " '1860',\n",
              " '18601920',\n",
              " '18601970',\n",
              " '18611914',\n",
              " '18621929',\n",
              " '186474',\n",
              " '1867',\n",
              " '1868',\n",
              " '18691958raven',\n",
              " '187',\n",
              " '18701940',\n",
              " '18701942',\n",
              " '1871the',\n",
              " '1872',\n",
              " '18731926',\n",
              " '1874',\n",
              " '1875',\n",
              " '18751878',\n",
              " '1876',\n",
              " '18761976',\n",
              " '1877',\n",
              " '18781965',\n",
              " '18791979',\n",
              " '188',\n",
              " '1880',\n",
              " '18801920',\n",
              " '18801930',\n",
              " '18801958',\n",
              " '18801980',\n",
              " '1880s',\n",
              " '1881',\n",
              " '188335',\n",
              " '1883in',\n",
              " '1884',\n",
              " '1885',\n",
              " '1886',\n",
              " '18861887',\n",
              " '1887',\n",
              " '188plinearmueller',\n",
              " '189',\n",
              " '1890s',\n",
              " '1890she',\n",
              " '1892',\n",
              " '189395112x',\n",
              " '1894',\n",
              " '18941979',\n",
              " '18951945',\n",
              " '1896',\n",
              " '18961977',\n",
              " '1897',\n",
              " '1898',\n",
              " '18981980',\n",
              " '1899',\n",
              " '18991942',\n",
              " '1899penrose',\n",
              " '189th',\n",
              " '18after',\n",
              " '18carat',\n",
              " '18disc',\n",
              " '18game',\n",
              " '18in',\n",
              " '18inch',\n",
              " '18month',\n",
              " '18o',\n",
              " '18qf2',\n",
              " '18s',\n",
              " '18th',\n",
              " '18thcentury',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYyRTa0rqEsQ",
        "colab_type": "text"
      },
      "source": [
        "Since most documents don't contain every word, many of the TF-IDF entries will be 0. Representing the TF-IDF matrix as a `numpy` matrix will require a lot of unnecessary storage to keep track of all those 0. SciPy provides the idea of a \"sparse matrix\" that only represents the non-zero entries of a matrix to save space. Externally, you treat it just like a numpy `matrix` but it takes up less storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW25G1_LqEsR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "838e06aa-1bc9-42f8-8e18-29aba02b48c4"
      },
      "source": [
        "tf_idf = csr_matrix(tf_idf)\n",
        "\n",
        "tf_idf.shape #5907 pages in the data set and each of the 112801 unique words.\n",
        "#print(tf_idf) #(document, word) ....    prob of seeing word? "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5907, 112801)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEbH9bvPqEsT",
        "colab_type": "text"
      },
      "source": [
        "The above matrix contains a TF-IDF score for each of the 5907 pages in the data set and each of the 112801 unique words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-LgP2iRqEsU",
        "colab_type": "text"
      },
      "source": [
        "# Normalize all vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjycwCRAqEsU",
        "colab_type": "text"
      },
      "source": [
        "As discussed in the previous assignment, Euclidean distance can be a poor metric of similarity between documents, as it unfairly penalizes long articles. For a reasonable assessment of similarity, we should disregard the length information and use length-agnostic metrics, such as cosine distance.\n",
        "\n",
        "The k-means algorithm does not directly work with cosine distance, so we take an alternative route to remove length information: we normalize all vectors to be unit length. It turns out that Euclidean distance closely mimics cosine distance when all vectors are unit length. In particular, the squared Euclidean distance between any two vectors of length one is directly proportional to their cosine distance.\n",
        "\n",
        "---\n",
        "*Optional:* This section has some optional background material as to why normalizing makes sense here. You can skip down to the next line break if you don't want to read this.\n",
        "\n",
        "We can prove this as follows. Let $\\mathbf{x}$ and $\\mathbf{y}$ be normalized vectors, i.e. unit vectors, so that $\\|\\mathbf{x}\\|=\\|\\mathbf{y}\\|=1$. Write the squared Euclidean distance as the dot product of $(\\mathbf{x} - \\mathbf{y})$ to itself:\n",
        "\\begin{align*}\n",
        "\\|\\mathbf{x} - \\mathbf{y}\\|_2^2 &= (\\mathbf{x} - \\mathbf{y})^T(\\mathbf{x} - \\mathbf{y}) & \\text{(def of L2 norm)}\\\\\n",
        "                              &= (\\mathbf{x}^T \\mathbf{x}) - 2(\\mathbf{x}^T \\mathbf{y}) + (\\mathbf{y}^T \\mathbf{y}) & \\text{(FOIL expression)}\\\\\n",
        "                              &= \\|\\mathbf{x}\\|_2^2 - 2(\\mathbf{x}^T \\mathbf{y}) + \\|\\mathbf{y}\\|_2^2 & \\text{(def of L2 norm)}\\\\\n",
        "                              &= 2 - 2(\\mathbf{x}^T \\mathbf{y}) & \\text{($\\mathbf{x}$ and $\\mathbf{y}$ are length 1)}\\\\\n",
        "                              &= 2(1 - (\\mathbf{x}^T \\mathbf{y}))\\\\\n",
        "                              &= 2\\left(1 - \\frac{\\mathbf{x}^T \\mathbf{y}}{\\|\\mathbf{x}\\|_2\\|\\mathbf{y}\\|_2}\\right) & \\text{(Dividing by 1 doesn't change value)}\\\\\n",
        "                              &= 2\\left[\\text{cosine distance}\\right]\n",
        "\\end{align*}\n",
        "\n",
        "This tells us that two **unit vectors** that are close in Euclidean distance are also close in cosine distance. Thus, the k-means algorithm (which naturally uses Euclidean distances) on normalized vectors will produce the same results as clustering using cosine distance as a distance metric.\n",
        "\n",
        "\n",
        "---\n",
        "We import the [`normalize()` function](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html) from scikit-learn to normalize all vectors to unit length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQKNfIcCqEsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "tf_idf = normalize(tf_idf)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7U_xrTwqEsX",
        "colab_type": "text"
      },
      "source": [
        "# Implement k-means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEfCjgiaqEsY",
        "colab_type": "text"
      },
      "source": [
        "Let us implement the k-means algorithm. First, we choose an initial set of centroids. A common practice is to choose randomly from the data points.\n",
        "\n",
        "**Note:** We specify a seed here, so that everyone gets the same answer. In practice, we highly recommend to use different seeds every time (for instance, by using the current timestamp)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnrPdZz9Ie3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#rand_indices = np.random.randint(0, 2, 3)\n",
        "#rand_indices #outcome = array([1, 1, 1])\n",
        "#type(rand_indices)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKSBAVTNqEsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_initial_centroids(data, k, seed=None):\n",
        "    \"\"\"\n",
        "    Randomly choose k data points as initial centroids\n",
        "    \"\"\"\n",
        "    if seed is not None: # useful for obtaining consistent results\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "    n = data.shape[0] # number of data points\n",
        "        \n",
        "    # Pick K indices from range [0, N).\n",
        "    rand_indices = np.random.randint(0, n, k)\n",
        "    \n",
        "    # Keep centroids as dense format, as many entries will be nonzero due to averaging.\n",
        "    # As long as at least one document in a cluster contains a word,\n",
        "    # it will carry a nonzero weight in the TF-IDF vector of the centroid.\n",
        "    centroids = data[rand_indices,:].toarray()\n",
        "    \n",
        "    return centroids"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v1FxFiXqEsb",
        "colab_type": "text"
      },
      "source": [
        "### k-means Algorithm\n",
        "After initialization, the k-means algorithm iterates between the following two steps:\n",
        "1. Assign each data point to the closest centroid. $$z_i \\gets \\mathrm{argmin}_j \\|\\mathbf{\\mu}_j - \\mathbf{x}_i\\|^2$$\n",
        "2. Revise centroids as the mean of the assigned data points. $$\\mathbf{\\mu}_j \\gets \\frac{1}{n_j}\\sum_{i:z_i=j} \\mathbf{x}_i$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV9NpZa5qEsc",
        "colab_type": "text"
      },
      "source": [
        "In pseudocode, we iteratively do the following:\n",
        "```python\n",
        "cluster_assignment = assign_clusters(data, centroids)\n",
        "centroids = revise_centroids(data, k, cluster_assignment)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODoiN4ALqEsd",
        "colab_type": "text"
      },
      "source": [
        "## Assigning clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT4A-Uu0qEse",
        "colab_type": "text"
      },
      "source": [
        "How do we implement Step 1 of the main k-means loop above? First we import `pairwise_distances` function from scikit-learn, which calculates Euclidean distances between rows of given arrays. See [this documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.paired_distances.html) for more information.\n",
        "\n",
        "For the sake of demonstration, let's look at documents 100 through 102 as query documents and compute the distances between each of these documents and every other document in the corpus. In the k-means algorithm, we will have to compute pairwise distances between the set of centroids and the set of documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETeFLowZqEsf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "210dfaff-5afa-4a8c-c3f6-67da10c53a4f"
      },
      "source": [
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "# Get the TF-IDF vectors for documents 100 through 102.\n",
        "queries = tf_idf[100:102,:]\n",
        "\n",
        "# Compute pairwise distances from every data point to each query vector.\n",
        "dist = pairwise_distances(tf_idf, queries, metric='euclidean')\n",
        "print(dist)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.39996239 1.39958932]\n",
            " [1.40386156 1.39754968]\n",
            " [1.38421176 1.39682604]\n",
            " ...\n",
            " [1.40562888 1.39024794]\n",
            " [1.39673862 1.38306708]\n",
            " [1.40872806 1.40250208]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHodIMqRqEsh",
        "colab_type": "text"
      },
      "source": [
        "More formally, `dist[i,j]` is assigned the distance between the `i`th row of `X` (i.e., `X[i,:]`) and the `j`th row of `Y` (i.e., `Y[j,:]`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Etm-JKYtqEsi",
        "colab_type": "text"
      },
      "source": [
        "**Checkpoint:** To test your understanding of how this code works, in the cell below write practice code that does the following tasks\n",
        "* Initializes 3 centroids that are the first 3 rows of `tf_idf`\n",
        "* Compute the distances between all the points in `tf_idf` and the 3 centroids. The result should be a matrix with shape `(5907, 3)`. Store this in a variable called `distances`.\n",
        "* Use `distances` to find the distance between the row of `tf_idf` with index 430 to the second centroid (index 1). Store this value in a variable called `dist`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBLbi7SBIrOE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "f4688747-954c-4b5c-fa76-96a5ce07ca38"
      },
      "source": [
        "#Try again ---\n",
        "#Try to emulate centroid assignment above. --get centroids\n",
        "indices = np.array([0, 1, 2])\n",
        "centroids = tf_idf[indices,:].toarray()\n",
        "#print(centroids)\n",
        "\n",
        "#Compute the distances between all the points in tf_idf and the 3 centroids. \n",
        "#The result should be a matrix with shape (5907, 3). Store this in a variable called distances\n",
        "distances = pairwise_distances(tf_idf, centroids, metric='euclidean')\n",
        "print(distances.shape)\n",
        "print(distances)\n",
        "\n",
        "#Use distances to find the distance between the row of tf_idf with index 430 \n",
        "#to the second centroid (index 1). Store this value in a variable called dist.\n",
        "#row = tf_idf[430,:]\n",
        "dist = distances[430][1]\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5907, 3)\n",
            "[[0.00000000e+00 1.40383275e+00 1.39333920e+00]\n",
            " [1.40383275e+00 1.49011612e-08 1.39553028e+00]\n",
            " [1.39333920e+00 1.39553028e+00 0.00000000e+00]\n",
            " ...\n",
            " [1.39157805e+00 1.39141281e+00 1.38192050e+00]\n",
            " [1.39962336e+00 1.38790661e+00 1.38090390e+00]\n",
            " [1.40691565e+00 1.40462283e+00 1.40161262e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvexWSzOqEsl",
        "colab_type": "text"
      },
      "source": [
        "For the first part of this assignment, we will provide cells labelled `# Test Cell` to run a small test on the code you just wrote to let you know if it was right or not! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKorIXvIqEsl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "173b0442-e339-4c0d-8d70-c4fac98970ea"
      },
      "source": [
        "# Test Cell\n",
        "if distances.shape == (5907, 3) and np.allclose(dist, pairwise_distances(tf_idf[430,:], tf_idf[1,:])):\n",
        "    print('Pass')\n",
        "else:\n",
        "    print('Check your code again')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pass\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrRAJSmkqEsn",
        "colab_type": "text"
      },
      "source": [
        "**Checkpoint:** Next, given the pairwise distances, we take the minimum of the distances for each data point. Fittingly, NumPy provides an `argmin` function. See [this documentation](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.argmin.html) for details. After reading the documentation, in the cell below, write code to produce a 1D array whose $i^{th}$ entry indicates the centroid index that is the closest to the $i^{th}$ data point. Use the list of distances from the previous checkpoint. Following the theme of this case study, we will judge whether the clustering makes sense in the context of document analysis. Save this array as `closest_cluster`.\n",
        "\n",
        "As a note, it would be very slow to compute if you do not use the `argmin` function and try to implement it like we do in the test cell below. If you want your notebook to run in a reasonable amount of time, you will need to use the `argmin` function.\n",
        "\n",
        "*Hint:* the resulting array should be as long as the number of data points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saLM4tUUqEso",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ce6b921-1ada-4dd1-e68c-0fd7a9626584"
      },
      "source": [
        "# TODO Fill out this cell\n",
        "#For 5907 items, have to fill in which of the centroids is closest. \n",
        "closest_cluster = np.argmin(distances, axis=1)\n",
        "len(closest_cluster) #good.\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5907"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6Cz89vkqEsq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "df2362a2-709a-478b-f0e9-2234f630bb49"
      },
      "source": [
        "# Test Cell\n",
        "reference = [list(row).index(min(row)) for row in distances]\n",
        "if np.allclose(closest_cluster, reference):\n",
        "    print('Pass')\n",
        "else:\n",
        "    print('Check your code again')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pass\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfJZ-KCEqEsu",
        "colab_type": "text"
      },
      "source": [
        "**Checkpoint:** Let's put these steps together.  First, initialize three centroids with the first 3 rows of `tf_idf`. Then, compute distances from each of the centroids to all data points in `tf_idf`. Finally, use these distance calculations to compute cluster assignments and assign them to `cluster_assignment`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yda0BuWfqEsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Students should write code here\n",
        "\n",
        "#Part1: \n",
        "#Try to emulate centroid assignment above. --get centroids\n",
        "indices = np.array([0, 1, 2])\n",
        "centroids = tf_idf[indices,:].toarray()\n",
        "\n",
        "\n",
        "#Compute the distances between all the points in tf_idf and the 3 centroids. \n",
        "#The result should be a matrix with shape (5907, 3). Store this in a variable called distances\n",
        "distances = pairwise_distances(tf_idf, centroids, metric='euclidean')\n",
        "\n",
        "\n",
        "#Use distances to find the distance between the row of tf_idf with index 430 \n",
        "#to the second centroid (index 1). Store this value in a variable called dist.\n",
        "#row = tf_idf[430,:]\n",
        "dist = distances[430][1]\n",
        "\n",
        "\n",
        "#Part2: \n",
        "closest_cluster = np.argmin(distances, axis=1)\n",
        "len(closest_cluster) #good.\n",
        "\n",
        "#New code: \n",
        "#Finally, use these distance calculations to compute cluster assignments\n",
        "# and assign them to cluster_assignment\n",
        "cluster_assignment = closest_cluster\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdHvP94AqEsw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43258fe6-662f-41de-f581-ad3986d37b4a"
      },
      "source": [
        "if len(cluster_assignment)==5907 and \\\n",
        "   np.array_equal(np.bincount(cluster_assignment), np.array([515,  440, 4952])):\n",
        "    print('Pass') # count number of data points for each cluster\n",
        "else:\n",
        "    print('Check your code again.')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pass\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXwxtXX2qEsz",
        "colab_type": "text"
      },
      "source": [
        "Now we are ready to fill in the blanks the function `assign_clusters(data, centroids)`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeXVNuXXqEsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO \n",
        "def assign_clusters(data, centroids):\n",
        "    \"\"\"\n",
        "    Parameters:  \n",
        "      - data      - is an np.array of float values of length N.  \n",
        "      - centroids - is an np.array of float values of length k.\n",
        "\n",
        "    Returns  \n",
        "      -  A np.array of length N where the ith index represents which centroid \n",
        "         data[i] was assigned to. The assignments range between the values 0, ..., k-1.\n",
        "    \"\"\"\n",
        "    # TODO \n",
        "    \n",
        "    #Compute the distances between all the points in tf_idf and the 3 centroids. \n",
        "    #The result should be a matrix with shape (5907, 3). Store this in a variable called distances\n",
        "    distances = pairwise_distances(data, centroids, metric='euclidean')\n",
        "\n",
        "    #Part2: \n",
        "    closest_cluster = np.argmin(distances, axis=1)\n",
        "    len(closest_cluster) #good.\n",
        "\n",
        "    #New code: \n",
        "    #Finally, use these distance calculations to compute cluster assignments\n",
        "    # and assign them to cluster_assignment\n",
        "    cluster_assignment = closest_cluster\n",
        "\n",
        "    return cluster_assignment\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf5dJXZaqEs2",
        "colab_type": "text"
      },
      "source": [
        "**Checkpoint**. For the last time, let us check if Step 1 was implemented correctly. With rows 0, 2, 4, and 6 of `tf_idf` as an initial set of centroids, we assign cluster labels to rows 0, 10, 20, ..., and 90 of `tf_idf`. The resulting cluster labels should be `[0, 3, 3, 3, 3, 2, 2, 1, 1, 1]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4CO6q8iqEs3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f0712b0-d8a5-4840-fe3f-9d7c7d96f966"
      },
      "source": [
        "if np.allclose(assign_clusters(tf_idf[0:100:10], tf_idf[0:8:2]), np.array([0, 3, 3, 3, 3, 2, 2, 1, 1, 1])):\n",
        "    print('Pass')\n",
        "else:\n",
        "    print('Check your code again.')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pass\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYZc94XqqEs8",
        "colab_type": "text"
      },
      "source": [
        "## Revising clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIzsCK3wqEs9",
        "colab_type": "text"
      },
      "source": [
        "Let's turn to Step 2 of the k-means algorithm, where we compute the new centroids given the cluster assignments. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lsrrqXiqEs9",
        "colab_type": "text"
      },
      "source": [
        "SciPy and NumPy arrays allow for filtering via Boolean masks. For instance, we filter all data points that are assigned to cluster 0 by writing\n",
        "```python\n",
        "data[cluster_assignment==0,:]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_PrsqP7qEs-",
        "colab_type": "text"
      },
      "source": [
        "To develop intuition about filtering, let's look at a toy example consisting of 3 data points and 2 clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zzdwq_UqEs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.array([[1., 2., 0.],\n",
        "                 [0., 0., 0.],\n",
        "                 [2., 2., 0.]])\n",
        "centroids = np.array([[0.5, 0.5, 0.],\n",
        "                      [0., -0.5, 0.]])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHFlqJVPqEtB",
        "colab_type": "text"
      },
      "source": [
        "Let's assign these data points to the closest centroid using the function you wrote before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7viM1VugqEtB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f80e0bb3-2db8-4d16-b17e-586b5de30140"
      },
      "source": [
        "cluster_assignment = assign_clusters(data, centroids)\n",
        "print(cluster_assignment)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqMWUcb9qEtD",
        "colab_type": "text"
      },
      "source": [
        "The expression `cluster_assignment==1` gives a list of Booleans that says whether each data point is assigned to cluster 1 or not:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3Y0bWvqqEtD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ada6860-f165-4af3-b468-3d87378f64e5"
      },
      "source": [
        "cluster_assignment==1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False,  True, False])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci7uTNTkqEtF",
        "colab_type": "text"
      },
      "source": [
        "Likewise for cluster 0:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T6AufasqEtF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ff508dc-cd05-4c4a-904e-ef6669c69fac"
      },
      "source": [
        "cluster_assignment==0"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True, False,  True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxcF3P81qEtH",
        "colab_type": "text"
      },
      "source": [
        "In lieu of indices, we can put in the list of Booleans to pick and choose rows. Only the rows that correspond to a `True` entry will be retained.\n",
        "\n",
        "First, let's look at the data points (i.e., their values) assigned to cluster 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilWPVQS5qEtH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "13a0cecd-a628-48de-ffc9-5ff06c66f2ad"
      },
      "source": [
        "data[cluster_assignment==1]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzsF6VmAqEtJ",
        "colab_type": "text"
      },
      "source": [
        "This makes sense since the vector `[0 0 0]` is closer to the centroid `[0 -0.5 0]` than to the centroid `[0.5 0.5 0]`.\n",
        "\n",
        "Now let's look at the data points assigned to cluster 0:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDsJb5x2qEtJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "115e4b0c-0e43-4f66-f410-502811081636"
      },
      "source": [
        "data[cluster_assignment==0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 2., 0.],\n",
              "       [2., 2., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhUj_DsmqEtL",
        "colab_type": "text"
      },
      "source": [
        "Again, this makes sense since these values are each closer to the centroid `[0.5 0.5 0]` than to `[0 -0.5 0]`.\n",
        "\n",
        "Given all the data points in a cluster, it only remains to compute the mean. Use [np.mean()](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.mean.html). By default, the function averages all elements in a 2D array. To compute row-wise or column-wise means, add the `axis` argument. See the linked documentation for details. \n",
        "\n",
        "In the cell below, we first find all the rows that were assigned cluster 0 and then take the average of those vectors to find the new cluster 0 centroid. Notice that the result will be an np.array with 3 elements because that's the dimensionality of the vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuwHx5AqqEtM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "299271ad-346b-4ff2-a040-6c4d1448fcdc"
      },
      "source": [
        "data[cluster_assignment==0].mean(axis=0)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.5, 2. , 0. ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpY4TRiPqEtO",
        "colab_type": "text"
      },
      "source": [
        "Now we are ready to fill in the blanks the function `revise_centroids(data, k, cluster_assignment)`. In the cell below, complete the `...` sections to compute the new centroids given the current cluster assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VnTum_RqEtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO\n",
        "def revise_centroids(data, k, cluster_assignment):\n",
        "    \"\"\"\n",
        "    Parameters:  \n",
        "      - data               - is an np.array of float values of length N.\n",
        "      - k                  - number of centroids\n",
        "      - cluster_assignment - np.array of length N where the ith index represents which \n",
        "                             centroid data[i] was assigned to. The assignments range between the values 0, ..., k-1.\n",
        "\n",
        "    Returns  \n",
        "      -  A np.array of length k for the new centroids.\n",
        "    \"\"\"\n",
        "    new_centroids = []\n",
        "    for i in range(k):\n",
        "        # Select all data points that belong to cluster i. Fill in the blank (RHS only)\n",
        "        member_data_points = data[cluster_assignment==i]\n",
        "        # Compute the mean of the data points. Fill in the blank (RHS only)\n",
        "        centroid = member_data_points.mean(axis=0)\n",
        "        \n",
        "        # Convert numpy.matrix type to numpy.ndarray type\n",
        "        centroid = centroid.A1\n",
        "        new_centroids.append(centroid)\n",
        "        \n",
        "    new_centroids = np.array(new_centroids)\n",
        "    return new_centroids"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZablY7S-qEtS",
        "colab_type": "text"
      },
      "source": [
        "**Checkpoint**. Let's check our Step 2 implementation. Letting rows 0, 10, ..., 100 of `tf_idf` as the data points and the cluster labels `[0, 1, 1, 0, 0, 2, 0, 2, 2, 1]`, we compute the next set of centroids. Each centroid is given by the average of all member data points in corresponding cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuC9ALEaqEtS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ab5c610-f587-4b68-bd79-3c14d17dbacd"
      },
      "source": [
        "result = revise_centroids(tf_idf[0:100:10], 3, np.array([0, 1, 1, 0, 0, 2, 0, 2, 2, 1]))\n",
        "if np.allclose(result[0], np.mean(tf_idf[[0,30,40,60]].toarray(), axis=0)) and \\\n",
        "   np.allclose(result[1], np.mean(tf_idf[[10,20,90]].toarray(), axis=0))   and \\\n",
        "   np.allclose(result[2], np.mean(tf_idf[[50,70,80]].toarray(), axis=0)):\n",
        "    print('Pass')\n",
        "else:\n",
        "    print('Check your code')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pass\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iySadBTFqEtU",
        "colab_type": "text"
      },
      "source": [
        "### Assessing convergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26Op9TPuqEtV",
        "colab_type": "text"
      },
      "source": [
        "How can we tell if the k-means algorithm is converging? We can look at the cluster assignments and see if they stabilize over time. In fact, we'll be running the algorithm until the cluster assignments stop changing at all. To be extra safe, and to assess the clustering performance, we'll be looking at an additional criteria: the sum of all squared distances between data points and centroids. This is defined as\n",
        "$$\n",
        "J(\\mathcal{Z},\\mu) = \\sum_{j=0}^{k-1} \\sum_{i=1:z_i = j}^n \\|\\mathbf{x}_i - \\mu_j\\|^2.\n",
        "$$\n",
        "The smaller the distances, the more homogeneous the clusters are. In other words, we'd like to have \"tight\" clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwD2YztbqEtV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_heterogeneity(data, k, centroids, cluster_assignment):\n",
        "    \"\"\"\n",
        "    Computes the heterogeneity metric of the data using the given centroids and cluster assignments.\n",
        "    \"\"\"\n",
        "    heterogeneity = 0.0\n",
        "    for i in range(k):\n",
        "        \n",
        "        # Select all data points that belong to cluster i. Fill in the blank (RHS only)\n",
        "        member_data_points = data[cluster_assignment==i,:]\n",
        "        \n",
        "        if member_data_points.shape[0] > 0: # check if i-th cluster is non-empty\n",
        "            # Compute distances from centroid to data point\n",
        "            distances = pairwise_distances(member_data_points, [centroids[i]], metric='euclidean')\n",
        "            squared_distances = distances**2\n",
        "            heterogeneity += np.sum(squared_distances)\n",
        "        \n",
        "    return heterogeneity"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOmGx-EHqEtX",
        "colab_type": "text"
      },
      "source": [
        "Let's compute the cluster heterogeneity for the 2-cluster example we've been considering based on our current cluster assignments and centroids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5DkruonqEtX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "beb05371-9fa7-4c36-9e94-081f0d613a79"
      },
      "source": [
        "compute_heterogeneity(data, 2, centroids, cluster_assignment)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpHq8_fkqEtZ",
        "colab_type": "text"
      },
      "source": [
        "### Combining into a single function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0kAOad1qEtZ",
        "colab_type": "text"
      },
      "source": [
        "Once the two k-means steps have been implemented, as well as our heterogeneity metric we wish to monitor, it is only a matter of putting these functions together to write a k-means algorithm that\n",
        "\n",
        "* Repeatedly performs Steps 1 and 2\n",
        "* Tracks convergence metrics\n",
        "* Stops if either no assignment changed or we reach a certain number of iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHaCOt70qEta",
        "colab_type": "text"
      },
      "source": [
        "Now we are ready to fill in the blanks the function `kmeans(data, k, initial_centroids, maxiter, record_heterogeneity=None, verbose=False)`. In the cell below, complete the `...` sections to meet the specification of the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QveqoxjJqEta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO Fill in the blanks\n",
        "def kmeans(data, k, initial_centroids, maxiter, record_heterogeneity=None, verbose=False):\n",
        "    \"\"\"\n",
        "    This function runs k-means on given data and initial set of centroids.\n",
        "    \n",
        "    Parameters:  \n",
        "      - data                 - is an np.array of float values of length N.\n",
        "      - k                    - number of centroids\n",
        "      - initial_centroids    - is an np.array of float values of length k.\n",
        "      - maxiter              - maximum number of iterations to run the algorithm\n",
        "      - record_heterogeneity - if provided an empty list, it will compute the heterogeneity \n",
        "                               at each iteration and append it to the list. \n",
        "                               Defaults to None and won't record heterogeneity.\n",
        "      - verbose              - set to True to display progress. Defaults to False and won't \n",
        "                               display progress.\n",
        "\n",
        "    Returns  \n",
        "      - centroids - A np.array of length k for the centroids upon termination of the algorithm.\n",
        "      - cluster_assignment - A np.array of length N where the ith index represents which \n",
        "                             centroid data[i] was assigned to. The assignments range between the \n",
        "                             values 0, ..., k-1 upon termination of the algorithm.\n",
        "    \"\"\"\n",
        "    centroids = initial_centroids[:]\n",
        "    prev_cluster_assignment = None\n",
        "    \n",
        "    for itr in range(maxiter):  \n",
        "        # Print itereation number\n",
        "        if verbose:\n",
        "            print(itr)\n",
        "        \n",
        "        # 1. Make cluster assignments using nearest centroids\n",
        "        distances = pairwise_distances(data, centroids, metric='euclidean')\n",
        "        cluster_assignment = np.argmin(distances, axis=1)\n",
        "  \n",
        "        # 2. Compute a new centroid for each of the k clusters, averaging all data points assigned to that cluster.\n",
        "        #member_data_points = data[cluster_assignment==k]\n",
        "        #print(\"member_data_points\")\n",
        "        #print(member_data_points)\n",
        "        #centroid = member_data_points.mean(axis=0)\n",
        "        #centroid = centroid.A1\n",
        "        centroids = revise_centroids(data, k, cluster_assignment)\n",
        "            \n",
        "        # Check for convergence: if none of the assignments changed, stop\n",
        "        if prev_cluster_assignment is not None and \\\n",
        "          (prev_cluster_assignment == cluster_assignment).all():\n",
        "            break\n",
        "        \n",
        "        # Print number of new assignments \n",
        "        if prev_cluster_assignment is not None:\n",
        "            num_changed = sum(abs(prev_cluster_assignment - cluster_assignment))\n",
        "            if verbose:\n",
        "                print(f'    {num_changed:5d} elements changed their cluster assignment.')  \n",
        "        \n",
        "        # Record heterogeneity convergence metric\n",
        "        if record_heterogeneity is not None:\n",
        "            score = compute_heterogeneity(data, k, centroids, cluster_assignment)\n",
        "            record_heterogeneity.append(score)\n",
        "        \n",
        "        prev_cluster_assignment = cluster_assignment[:]\n",
        "        \n",
        "    return centroids, cluster_assignment"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDWls7MoqEtc",
        "colab_type": "text"
      },
      "source": [
        "## Plotting convergence metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyZlI6lnqEtc",
        "colab_type": "text"
      },
      "source": [
        "We can use the above function to plot the convergence metric across iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WiZ0uJPqEtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_heterogeneity(heterogeneity, k):\n",
        "    \"\"\"\n",
        "    Plots how the heterogeneity changes as the number of iterations increases.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(7,4))\n",
        "    plt.plot(heterogeneity, linewidth=4)\n",
        "    plt.xlabel('# Iterations')\n",
        "    plt.ylabel('Heterogeneity')\n",
        "    plt.title(f'Heterogeneity of clustering over time, K={k}')\n",
        "    plt.rcParams.update({'font.size': 16})\n",
        "    plt.tight_layout()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIqZMfCLqEte",
        "colab_type": "text"
      },
      "source": [
        "Let's consider running k-means with K=3 clusters for a maximum of 400 iterations, recording cluster heterogeneity at every step.  Then, let's plot the heterogeneity over iterations using the plotting function above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMu3ItP7qEtf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "fafacbc4-1aba-48d2-e400-b7fb1301c070"
      },
      "source": [
        "k = 3\n",
        "heterogeneity = []\n",
        "initial_centroids = get_initial_centroids(tf_idf, k, seed=0)\n",
        "centroids, cluster_assignment = kmeans(tf_idf, k, initial_centroids, maxiter=400,\n",
        "                                       record_heterogeneity=heterogeneity, verbose=True)\n",
        "plot_heterogeneity(heterogeneity, k)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "     2036 elements changed their cluster assignment.\n",
            "2\n",
            "      654 elements changed their cluster assignment.\n",
            "3\n",
            "      376 elements changed their cluster assignment.\n",
            "4\n",
            "      264 elements changed their cluster assignment.\n",
            "5\n",
            "      112 elements changed their cluster assignment.\n",
            "6\n",
            "       37 elements changed their cluster assignment.\n",
            "7\n",
            "       10 elements changed their cluster assignment.\n",
            "8\n",
            "        6 elements changed their cluster assignment.\n",
            "9\n",
            "        2 elements changed their cluster assignment.\n",
            "10\n",
            "        2 elements changed their cluster assignment.\n",
            "11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ77YdkvqEth",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    <h4>Question 1</h4> \n",
        "    <p>\n",
        "        In this example, the clustering objective (heterogeneity) is non-increasing. \n",
        "    </p>\n",
        "    <p><i>Note: For this problem you don't need to write any code except you do need to run the code above to make the plot</i></p>\n",
        "    <p>\n",
        "        <b>Gradescope:</b> Select one choice.\n",
        "    </p>\n",
        "    <ul>\n",
        "        <li>True</li>\n",
        "        <li>False</li>\n",
        "    </ul>\n",
        "</div>\n",
        "\n",
        "<div class=\"alert alert-block alert-success\">\n",
        "    <h4>Question 2</h4> \n",
        "    <p>\n",
        "        In general, if the heterogeneity objective would ever increase when running k-means, that would indicate:\n",
        "    </p>\n",
        "    <p><i>Note: For this problem you don't need to write any code except you do need to run the code above to make the plot</i></p>\n",
        "    <p>\n",
        "        <b>Gradescope:</b> Select one choice.\n",
        "    </p>\n",
        "    <ul>\n",
        "        <li>k-means algorithm got stuck in a bad local minimum</li>\n",
        "        <li>There is a bug in the k-means code</li>\n",
        "        <li>All data points consist of exact duplicates</li>\n",
        "        <li>Nothing is wrong. The objective should generally go down sooner or later.</li>\n",
        "    </ul>\n",
        "</div>\n",
        "\n",
        "<div class=\"alert alert-block alert-success\">\n",
        "    <h4>Question 3</h4> \n",
        "    <p>\n",
        "        Which of the cluster contains the greatest number of data points in the end?  \n",
        "        <i>Hint:</i> Use <code>np.bincount</code> (documentation <a href=\"http://docs.scipy.org/doc/numpy-1.11.0/reference/generated/numpy.bincount.html\">here</a>) to count occurrences of each cluster label. Write your solution in the cell below.\n",
        "    </p>\n",
        "    <p>\n",
        "        <b>Gradescope:</b> Select one choice.\n",
        "    </p>\n",
        "    <ul>\n",
        "        <li>Cluster #0</li>\n",
        "        <li>Cluster #1</li>\n",
        "        <li>Cluster #2</li>\n",
        "    </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18SGZe9FqEti",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ee21913-5be2-46c1-c4b4-1e906a2cac8c"
      },
      "source": [
        "# TODO\n",
        "# Which of the cluster contains the greatest number of data points in the end?\n",
        "np.bincount(cluster_assignment)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4689,  404,  814])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAmuWS8wqEtm",
        "colab_type": "text"
      },
      "source": [
        "# Beware of Local Minima"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC6Zs5_-qEtn",
        "colab_type": "text"
      },
      "source": [
        "One weakness of k-means is that it tends to get stuck in a local minimum based on its starting position. To see this, let us run k-means multiple times, with different initial centroids created using different random seeds.\n",
        "\n",
        "**Note:** Again, in practice, you should set different seeds for every run. We give you a list of seeds for this assignment so that everyone gets the same answer.\n",
        "\n",
        "This may take a minute or two to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mprx1wK6qEtn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "de6b6a8f-0b59-41ae-98af-8f12ff31a483"
      },
      "source": [
        "%%time\n",
        "# ^ Magic command to time how long it takes for this cell to run!\n",
        "# You can see how long it took with the output that says \"Wall time\"\n",
        "\n",
        "k = 10\n",
        "heterogeneity = {}\n",
        "for seed in [0, 20000, 40000, 60000, 80000, 100000, 120000]:\n",
        "    initial_centroids = get_initial_centroids(tf_idf, k, seed)\n",
        "    centroids, cluster_assignment = kmeans(tf_idf, k, initial_centroids, maxiter=400,\n",
        "                                           record_heterogeneity=None, verbose=False)\n",
        "    # To save time, compute heterogeneity only once in the end\n",
        "    heterogeneity[seed] = compute_heterogeneity(tf_idf, k, centroids, cluster_assignment)\n",
        "    print(f'seed={seed:06d}, heterogeneity={heterogeneity[seed]:.5f}')\n",
        "    \n",
        "    sys.stdout.flush()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seed=000000, heterogeneity=5569.17352\n",
            "seed=020000, heterogeneity=5563.93396\n",
            "seed=040000, heterogeneity=5562.09533\n",
            "seed=060000, heterogeneity=5574.80813\n",
            "seed=080000, heterogeneity=5563.38377\n",
            "seed=100000, heterogeneity=5565.93209\n",
            "seed=120000, heterogeneity=5572.40064\n",
            "CPU times: user 10.3 s, sys: 137 ms, total: 10.4 s\n",
            "Wall time: 10.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB-6CdxkqEtp",
        "colab_type": "text"
      },
      "source": [
        "Notice the variation in heterogeneity for different initializations. This indicates that k-means sometimes gets stuck at a bad local minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEEg_0RFqEtp",
        "colab_type": "text"
      },
      "source": [
        "One effective way to counter this tendency is to use **k-means++** to provide a smart initialization. This method tries to spread out the initial set of centroids so that they are not too close together. It is known to improve the quality of local optima and lower average runtime, but is a bit slower to start since it needs to do more computation to place centroids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWaf2ETPqEtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def smart_initialize(data, k, seed=None):\n",
        "    \"\"\"\n",
        "    Use k-means++ to initialize a good set of centroids\n",
        "    \"\"\"\n",
        "    if seed is not None: # useful for obtaining consistent results\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "    centroids = np.zeros((k, data.shape[1]))\n",
        "    \n",
        "    # Randomly choose the first centroid.\n",
        "    # Since we have no prior knowledge, choose uniformly at random\n",
        "    idx = np.random.randint(data.shape[0])\n",
        "    centroids[0] = data[idx,:].toarray()\n",
        "    \n",
        "    # Compute distances from the first centroid chosen to all the other data points\n",
        "    distances = pairwise_distances(data, centroids[0:1], metric='euclidean').flatten()\n",
        "    \n",
        "    for i in range(1, k):\n",
        "        # Choose the next centroid randomly, so that the probability for each data point to be chosen\n",
        "        # is directly proportional to its squared distance from the nearest centroid.\n",
        "        # Roughtly speaking, a new centroid should be as far as from ohter centroids as possible.\n",
        "        idx = np.random.choice(data.shape[0], 1, p=distances/sum(distances))\n",
        "        centroids[i] = data[idx,:].toarray()\n",
        "        \n",
        "        # Now compute distances from the centroids to all data points\n",
        "        distances = np.min(pairwise_distances(data, centroids[0:i+1], metric='euclidean'),axis=1)\n",
        "    \n",
        "    return centroids"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJFcP_b6qEts",
        "colab_type": "text"
      },
      "source": [
        "Let's now rerun k-means with 10 clusters using the same set of seeds, but always using k-means++ to initialize the algorithm.\n",
        "\n",
        "This may take several minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBK4WvjGqEts",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "49304b09-a0d8-43b7-c033-2306a14336d5"
      },
      "source": [
        "%%time\n",
        "\n",
        "k = 10\n",
        "heterogeneity_smart = {}\n",
        "for seed in [0, 20000, 40000, 60000, 80000, 100000, 120000]:\n",
        "    initial_centroids = smart_initialize(tf_idf, k, seed)\n",
        "    centroids, cluster_assignment = kmeans(tf_idf, k, initial_centroids, maxiter=400,\n",
        "                                           record_heterogeneity=None, verbose=False)\n",
        "    # To save time, compute heterogeneity only once in the end\n",
        "    heterogeneity_smart[seed] = compute_heterogeneity(tf_idf, k, centroids, cluster_assignment)\n",
        "    print(f'seed={seed:06d}, heterogeneity={heterogeneity_smart[seed]:.5f}')\n",
        "\n",
        "    sys.stdout.flush()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seed=000000, heterogeneity=5563.24947\n",
            "seed=020000, heterogeneity=5569.53006\n",
            "seed=040000, heterogeneity=5563.95996\n",
            "seed=060000, heterogeneity=5567.74848\n",
            "seed=080000, heterogeneity=5557.53663\n",
            "seed=100000, heterogeneity=5559.45113\n",
            "seed=120000, heterogeneity=5559.02003\n",
            "CPU times: user 11.1 s, sys: 131 ms, total: 11.2 s\n",
            "Wall time: 11.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WSvVWorqEtu",
        "colab_type": "text"
      },
      "source": [
        "Let's compare the set of cluster heterogeneities we got from our 7 restarts of k-means using random initialization compared to the 7 restarts of k-means using k-means++ as a smart initialization.\n",
        "\n",
        "The following code produces a [box plot](http://matplotlib.org/api/pyplot_api.html) for each of these methods, indicating the spread of values produced by each method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgKnKieXqEtu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "e3f6b37c-77a4-4c02-de75-335afcf1f6bb"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.boxplot([list(heterogeneity.values()), list(heterogeneity_smart.values())], vert=False)\n",
        "plt.yticks([1, 2], ['k-means', 'k-means++'])\n",
        "plt.rcParams.update({'font.size': 16})\n",
        "plt.xlabel('Heterogeneity')\n",
        "plt.tight_layout()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUF2otRSqEtx",
        "colab_type": "text"
      },
      "source": [
        "A few things to notice from the box plot:\n",
        "* Random initialization results in a worse clustering than k-means++ on average.\n",
        "* The best result of k-means++ is better than the best result of random initialization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm7toLMZqEtx",
        "colab_type": "text"
      },
      "source": [
        "**In general, you should run k-means at least a few times with different initializations and then return the run resulting in the lowest heterogeneity.** Let us write a function that runs k-means multiple times and picks the best run that minimizes heterogeneity. The function accepts an optional list of seed values to be used for the multiple runs; if no such list is provided, the current UTC time is used as seed values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQDXq3X3qEty",
        "colab_type": "text"
      },
      "source": [
        "Now we are ready to fill in the blanks the function `kmeans_multiple_runs(data, k, maxiter, verbose=False)`. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha9ga5pnqEty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO\n",
        "def kmeans_multiple_runs(data, k, maxiter, seeds, verbose=False):\n",
        "    \"\"\"\n",
        "    Runs kmeans multiple times \n",
        "    \n",
        "    Parameters:  \n",
        "      - data    - is an np.array of float values of length N.\n",
        "      - k       - number of centroids\n",
        "      - maxiter - maximum number of iterations to run the algorithm\n",
        "      - seeds   - Either number of seeds to try (generated randomly) or a list of seed values\n",
        "      - verbose - set to True to display progress. Defaults to False and won't display progress.\n",
        "    \n",
        "    Returns  \n",
        "      - final_centroids          - A np.array of length k for the centroids upon \n",
        "                                   termination of the algorithm.\n",
        "      - final_cluster_assignment - A np.array of length N where the ith index represents which \n",
        "                                   centroid data[i] was assigned to. The assignments range between \n",
        "                                   the values 0, ..., k-1 upon termination of the algorithm.\n",
        "    \"\"\"    \n",
        "    min_heterogeneity_achieved = float('inf')\n",
        "    final_centroids = None\n",
        "    final_cluster_assignment = None\n",
        "    if type(seeds) == int:\n",
        "        seeds = np.random.randint(low=0, high=10000, size=seeds)\n",
        "    \n",
        "    num_runs = len(seeds)\n",
        "    \n",
        "    for seed in seeds:\n",
        "        \n",
        "        # Use k-means++ initialization: Fill in the blank\n",
        "        # Set record_heterogeneity=None because we will compute that once at the end.\n",
        "        initial_centroids = smart_initialize(data, k, seed)\n",
        "        \n",
        "        # Run k-means: Fill in the blank \n",
        "        centroids, cluster_assignment = kmeans(data, k, initial_centroids, maxiter=400,\n",
        "                                           record_heterogeneity=None, verbose=False)\n",
        "        \n",
        "        # To save time, compute heterogeneity only once in the end\n",
        "        # Fill in the blank on the right\n",
        "        seed_heterogeneity = compute_heterogeneity(data, k, centroids, cluster_assignment)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f'seed={seed:06d}, heterogeneity={seed_heterogeneity:.5f}')\n",
        "            sys.stdout.flush()\n",
        "        \n",
        "        # if current measurement of heterogeneity is lower than previously seen,\n",
        "        # update the minimum record of heterogeneity.\n",
        "        if seed_heterogeneity < min_heterogeneity_achieved:\n",
        "            min_heterogeneity_achieved = seed_heterogeneity\n",
        "            final_centroids = centroids\n",
        "            final_cluster_assignment = cluster_assignment\n",
        "    \n",
        "    # Return the centroids and cluster assignments that minimize heterogeneity.\n",
        "    return final_centroids, final_cluster_assignment"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1lgVAVLqEtz",
        "colab_type": "text"
      },
      "source": [
        "## How to choose K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDL8IXz4qEt0",
        "colab_type": "text"
      },
      "source": [
        "Since we are measuring the tightness of the clusters, a higher value of K reduces the possible heterogeneity metric by definition.  For example, if we have N data points and set K=N clusters, then we could have 0 cluster heterogeneity by setting the N centroids equal to the values of the N data points. (Note: Not all runs for larger K will result in lower heterogeneity than a single run with smaller K due to local optima.)  Let's explore this general trend for ourselves by performing the following analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l337zL21qEt0",
        "colab_type": "text"
      },
      "source": [
        "Use the `kmeans_multiple_runs` function to run k-means with five different values of K.  For each K, use k-means++ and multiple runs to pick the best solution.  In what follows, we consider K=2,10,25,50,100 and 7 restarts for each setting.\n",
        "\n",
        "\n",
        "**IMPORTANT: The code block below will take about 5-6 minutes to finish.** It will try 5 values of `k` and for each `k`, will try 3 different seeds. The cell will print its progress to help you know how far it has made. When `k` is larger, it will take longer to run (why might that be?)!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNha4kkvqEt1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "982fc2e0-196f-417e-b459-4a055986c30c"
      },
      "source": [
        "%%time\n",
        "\n",
        "def plot_k_vs_heterogeneity(k_values, heterogeneity_values):\n",
        "    \"\"\"\n",
        "    Given list of k-values and their heterogeneities, will make a plot\n",
        "    showing how heterogeneity varies with k.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(7,4))\n",
        "    plt.plot(k_values, heterogeneity_values, linewidth=4)\n",
        "    plt.xlabel('K')\n",
        "    plt.ylabel('Heterogeneity')\n",
        "    plt.title('K vs. Heterogeneity')\n",
        "    plt.rcParams.update({'font.size': 16})\n",
        "    plt.tight_layout()\n",
        "\n",
        "all_centroids = {}\n",
        "all_cluster_assignment = {}\n",
        "heterogeneity_values = []\n",
        "seeds = [20000, 40000, 80000]\n",
        "k_list = [2, 10, 25, 50, 100]\n",
        "\n",
        "for k in k_list:\n",
        "    print(f'Running k = {k}')\n",
        "    heterogeneity = []\n",
        "    all_centroids[k], all_cluster_assignment[k] = kmeans_multiple_runs(tf_idf, k, maxiter=400,\n",
        "                                                                       seeds=seeds, verbose=True)\n",
        "    score = compute_heterogeneity(tf_idf, k, all_centroids[k], all_cluster_assignment[k])\n",
        "    heterogeneity_values.append(score)\n",
        "\n",
        "plot_k_vs_heterogeneity(k_list, heterogeneity_values)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running k = 2\n",
            "seed=020000, heterogeneity=5668.36228\n",
            "seed=040000, heterogeneity=5668.36228\n",
            "seed=080000, heterogeneity=5668.36228\n",
            "Running k = 10\n",
            "seed=020000, heterogeneity=5569.53006\n",
            "seed=040000, heterogeneity=5563.95996\n",
            "seed=080000, heterogeneity=5557.53663\n",
            "Running k = 25\n",
            "seed=020000, heterogeneity=5480.00474\n",
            "seed=040000, heterogeneity=5486.59028\n",
            "seed=080000, heterogeneity=5490.82007\n",
            "Running k = 50\n",
            "seed=020000, heterogeneity=5406.00305\n",
            "seed=040000, heterogeneity=5399.31884\n",
            "seed=080000, heterogeneity=5415.92085\n",
            "Running k = 100\n",
            "seed=020000, heterogeneity=5323.74445\n",
            "seed=040000, heterogeneity=5326.32351\n",
            "seed=080000, heterogeneity=5316.97425\n",
            "CPU times: user 1min 37s, sys: 1.15 s, total: 1min 38s\n",
            "Wall time: 1min 38s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sXShf7ZqEt3",
        "colab_type": "text"
      },
      "source": [
        "## Visualize clusters of documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ah5tJcZqEt3",
        "colab_type": "text"
      },
      "source": [
        "Let's start visualizing some clustering results to see if we think the clustering makes sense.  We can use such visualizations to help us assess whether we have set K too large or too small for a given application.  Following the theme of this course, we will judge whether the clustering makes sense in the context of document analysis.\n",
        "\n",
        "What are we looking for in a good clustering of documents?\n",
        "* Documents in the same cluster should be similar.\n",
        "* Documents from different clusters should be less similar.\n",
        "\n",
        "So a bad clustering exhibits either of two symptoms:\n",
        "* Documents in a cluster have mixed content.\n",
        "* Documents with similar content are divided up and put into different clusters.\n",
        "\n",
        "To help visualize the clustering, we do the following:\n",
        "* Fetch nearest neighbors of each centroid from the set of documents assigned to that cluster. We will consider these documents as being representative of the cluster.\n",
        "* Print titles and first sentences of those nearest neighbors.\n",
        "* Print top 5 words that have highest tf-idf weights in each centroid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOYSYztKqEt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_document_clusters(wiki, tf_idf, centroids, cluster_assignment, k, words, \n",
        "                                display_docs=5):\n",
        "    \"\"\"\n",
        "    Given a set of clustered documents, prints information about the centroids including\n",
        "       - The title and starting sentence of the closest 5 points to each centroid\n",
        "       - The five words that are contained in the clusters documents with the highest TF-IDF.\n",
        "    \n",
        "    Parameters:  \n",
        "      - wiki: original dataframe\n",
        "      - tf_idf: data matrix containing TF-IDF vectors for each document\n",
        "      - centroids: A np.array of length k that contains the centroids for the clustering\n",
        "      - cluster_assignments: A np.array of length N that has the cluster assignments for each row\n",
        "      - k: What value of k is used\n",
        "      - words: List of words in the corpus (should match tf_idf)\n",
        "      - display_odcs: How many documents to show for each cluster (default 5)\n",
        "    \"\"\"\n",
        "    print('=' * 90)\n",
        "\n",
        "    # Visualize each cluster c\n",
        "    for c in range(k):\n",
        "        # Cluster heading\n",
        "        print(f'Cluster {c}  ({(cluster_assignment == c).sum()} docs)'),\n",
        "        # Print top 5 words with largest TF-IDF weights in the cluster\n",
        "        idx = centroids[c].argsort()[::-1]\n",
        "        for i in range(5): # Print each word along with the TF-IDF weight\n",
        "            print(f'{words[idx[i]]}:{centroids[c,idx[i]]:.3f}', end=' '),\n",
        "        print()\n",
        "        \n",
        "        if display_docs > 0:\n",
        "            print()\n",
        "            # Compute distances from the centroid to all data points in the cluster,\n",
        "            # and compute nearest neighbors of the centroids within the cluster.\n",
        "            distances = pairwise_distances(tf_idf, centroids[c].reshape(1, -1), metric='euclidean').flatten()\n",
        "            distances[cluster_assignment!=c] = float('inf') # remove non-members from consideration\n",
        "            nearest_neighbors = distances.argsort()\n",
        "            # For the nearest neighbors, print the title as well as first 180 characters of text.\n",
        "            # Wrap the text at 80-character mark.\n",
        "            for i in range(display_docs):\n",
        "                text = ' '.join(wiki.iloc[nearest_neighbors[i]]['text'].split(None, 25)[0:25])\n",
        "                print(f'* {wiki.iloc[nearest_neighbors[i]][\"name\"]:50s} {distances[nearest_neighbors[i]]:.5f}')\n",
        "                print(f'  {text[:90]}')\n",
        "                if len(text) > 90:\n",
        "                    print(f'  {text[90:180]}')\n",
        "                print()\n",
        "        print('=' * 90)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Db4HxJaqEt5",
        "colab_type": "text"
      },
      "source": [
        "Let us first look at the 2 cluster case (K=2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI9VSXJxqEt5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "39892009-d518-4cff-e4ce-847d0b86b71d"
      },
      "source": [
        "k = 2\n",
        "visualize_document_clusters(wiki, tf_idf, all_centroids[k], all_cluster_assignment[k], k, words)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "Cluster 0  (1055 docs)\n",
            "she:0.169 her:0.117 was:0.040 for:0.038 as:0.033 \n",
            "\n",
            "* Bhama Srinivasan                                   0.89235\n",
            "  bhama srinivasan april 22 1935 is a mathematician known for her work in the representation\n",
            "   theory of finite groups her contributions were honored with the\n",
            "\n",
            "* Delores Brumfield                                  0.89849\n",
            "  delores brumfield white born may 26 1932 is a former utility infielderoutfielder who playe\n",
            "  d from 1947 through 1953 in the allamerican girls professional baseball league\n",
            "\n",
            "* Natashia Williams                                  0.90086\n",
            "  natashia williamsblach born august 2 1978 is an american actress and former wonderbra camp\n",
            "  aign model who is perhaps best known for her role as shane\n",
            "\n",
            "* Gila Golan                                         0.90518\n",
            "  gila golan hebrew born 1940 is a polishborn israeli former fashion model and actressgolan \n",
            "  was born in krakw poland around 1940 her exact birthday is\n",
            "\n",
            "* Bette McLaurin                                     0.90751\n",
            "  bette mclaurin born c1929 is an africanamerican singer best known for her jazzinfluenced b\n",
            "  allad and rb performances in the 1950s two of her recordings i\n",
            "\n",
            "==========================================================================================\n",
            "Cluster 1  (4852 docs)\n",
            "he:0.075 his:0.042 was:0.040 for:0.038 as:0.033 \n",
            "\n",
            "* Wilson McLean                                      0.94057\n",
            "  wilson mclean born 1937 is a scottish illustrator and artist he has illustrated primarily \n",
            "  in the field of advertising but has also provided cover art\n",
            "\n",
            "* Vipin Sharma                                       0.94801\n",
            "  vipin sharma is an indian actor born in new delhi he is a graduate of national school of d\n",
            "  rama new delhi india and the canadian\n",
            "\n",
            "* Nicky Banger                                       0.95174\n",
            "  nicholas lee banger born 25 february 1971 is a retired english professional footballer he \n",
            "  is currently head of commercial operations with woking fcbanger was born\n",
            "\n",
            "* Billy Bingham                                      0.95206\n",
            "  william laurence billy bingham mbe born 5 august 1931 is a former international footballer\n",
            "   and football manager who now works as a scout for english\n",
            "\n",
            "* Wally Whitehurst                                   0.95285\n",
            "  walter richard whitehurst born april 11 1964 in shreveport louisiana is a former righthand\n",
            "  ed pitcher in major league baseball who played from 1989 to 1996\n",
            "\n",
            "==========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipEHA_0tqEt7",
        "colab_type": "text"
      },
      "source": [
        "Both clusters have mixed content, although clearly cluster 0 are all women and cluster 1 are all men:\n",
        "\n",
        "It would be better if we sub-divided into more categories. So let us use more clusters. How about `K=10`?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MhpIgIF_qEt8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "09a1ae2b-fa8a-44ba-d79b-562849328ecd"
      },
      "source": [
        "k = 10\n",
        "visualize_document_clusters(wiki, tf_idf, all_centroids[k], all_cluster_assignment[k], k, words)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "Cluster 0  (872 docs)\n",
            "she:0.181 her:0.126 was:0.043 for:0.039 as:0.033 \n",
            "\n",
            "* Bhama Srinivasan                                   0.88835\n",
            "  bhama srinivasan april 22 1935 is a mathematician known for her work in the representation\n",
            "   theory of finite groups her contributions were honored with the\n",
            "\n",
            "* Delores Brumfield                                  0.89388\n",
            "  delores brumfield white born may 26 1932 is a former utility infielderoutfielder who playe\n",
            "  d from 1947 through 1953 in the allamerican girls professional baseball league\n",
            "\n",
            "* Natashia Williams                                  0.89781\n",
            "  natashia williamsblach born august 2 1978 is an american actress and former wonderbra camp\n",
            "  aign model who is perhaps best known for her role as shane\n",
            "\n",
            "* Gila Golan                                         0.90040\n",
            "  gila golan hebrew born 1940 is a polishborn israeli former fashion model and actressgolan \n",
            "  was born in krakw poland around 1940 her exact birthday is\n",
            "\n",
            "* Jane Jacobs (baseball)                             0.90399\n",
            "  jane jeanette jacobs badini born june 16 1924 is a former pitcher who played from 1944 thr\n",
            "  ough 1947 in the allamerican girls professional baseball league\n",
            "\n",
            "==========================================================================================\n",
            "Cluster 1  (435 docs)\n",
            "he:0.088 league:0.078 season:0.060 baseball:0.051 coach:0.050 \n",
            "\n",
            "* Wally Whitehurst                                   0.90524\n",
            "  walter richard whitehurst born april 11 1964 in shreveport louisiana is a former righthand\n",
            "  ed pitcher in major league baseball who played from 1989 to 1996\n",
            "\n",
            "* Sixto Lezcano                                      0.90576\n",
            "  sixto joaquin lezcano curras born november 28 1953 in arecibo puerto rico is a retired bas\n",
            "  eball player who played for 12 seasons as an outfielder\n",
            "\n",
            "* Joe Strong                                         0.90689\n",
            "  joseph benjamin strong born september 9 1962 in fairfield california is a former major lea\n",
            "  gue baseball pitcher who played for the florida marlins from 2000\n",
            "\n",
            "* Clyde Mashore                                      0.90856\n",
            "  clyde wayne mashore born may 29 1945 in concord california is a former major league baseba\n",
            "  ll outfielder who played in 241 games over five seasons\n",
            "\n",
            "* Dom Zanni                                          0.90946\n",
            "  dominick thomas zanni born march 1 1932 in the bronx new york is a former professional bas\n",
            "  eball player a righthanded pitcher he pitched in 111\n",
            "\n",
            "==========================================================================================\n",
            "Cluster 2  (642 docs)\n",
            "he:0.075 university:0.069 research:0.052 at:0.047 professor:0.042 \n",
            "\n",
            "* Elijah Anderson                                    0.92509\n",
            "  elijah anderson is an american sociologist he holds the william k lanman jr professorship \n",
            "  in sociology at yale university where he teaches and directs the\n",
            "\n",
            "* Sal Restivo                                        0.92838\n",
            "  sal restivo born 1940 is a sociologistanthropologist he is a leading contributor to scienc\n",
            "  e studies and in particular to the sociology of mathematical knowledge his\n",
            "\n",
            "* Stuart Henry (criminologist)                       0.93238\n",
            "  stuart henry is professor of criminal justice and director of the school of public affairs\n",
            "   san diego state university 2006 he has also been appointed\n",
            "\n",
            "* Daniel Berg (educator)                             0.93330\n",
            "  daniel berg is a scientist educator and was the fifteenth president of rensselaer polytech\n",
            "  nic institutehe was born on june 1 1929 in new york city\n",
            "\n",
            "* Robert Pollack (biologist)                         0.93396\n",
            "  dr robert pollack is an american biologist who studies the intersections between science a\n",
            "  nd religion he currently works at columbia university where he serves as\n",
            "\n",
            "==========================================================================================\n",
            "Cluster 3  (399 docs)\n",
            "he:0.095 football:0.069 league:0.062 club:0.061 his:0.057 \n",
            "\n",
            "* Tony Smith (footballer, born 1957)                 0.89297\n",
            "  anthony tony smith born 20 february 1957 is a former footballer who played as a central de\n",
            "  fender in the football league in the 1970s and\n",
            "\n",
            "* Steve Bruce                                        0.90494\n",
            "  stephen roger steve bruce born 31 december 1960 is an english football manager and former \n",
            "  player who is currently the manager at hull city born\n",
            "\n",
            "* Shane Hobbs                                        0.90966\n",
            "  shane hobbs born 30 april 1985 is a former professional footballer who played in the footb\n",
            "  all league for bristol rovers in 2003 and currently plays\n",
            "\n",
            "* Billy Bingham                                      0.91052\n",
            "  william laurence billy bingham mbe born 5 august 1931 is a former international footballer\n",
            "   and football manager who now works as a scout for english\n",
            "\n",
            "* Roy Keane                                          0.91080\n",
            "  royston maurice roy keane born 10 august 1971 is an irish football manager and former prof\n",
            "  essional football player he is the assistant manager of the\n",
            "\n",
            "==========================================================================================\n",
            "Cluster 4  (1286 docs)\n",
            "he:0.057 his:0.045 with:0.035 on:0.034 for:0.034 \n",
            "\n",
            "* Graham Ord                                         0.94522\n",
            "  graham ord born 22 march 1961 is an english musician and songwriter he has garnered respec\n",
            "  t internationally as a fine musician and engaging communicator his\n",
            "\n",
            "* Mark Wilkinson (singer)                            0.94764\n",
            "  mark wilkinson singer born buckinghamshire england is an australian country singersongwrit\n",
            "  er whose lyrical depth gift for melody and impassioned delivery are quickly establishing h\n",
            "\n",
            "* Richard Warren (musician)                          0.94880\n",
            "  richard daniel warren born 3 june 1973 is a british musician songwriter and producerhe sig\n",
            "  ned his first record deal with heavenly records at the height\n",
            "\n",
            "* Stewart Levine                                     0.95124\n",
            "  stewart levine is an american record producer he has worked with such artists as the crusa\n",
            "  ders minnie riperton lionel richie simply red hugh masekela dr\n",
            "\n",
            "* Josh Wilson (musician)                             0.95582\n",
            "  joshua david josh wilson born november 14 1983 is a contemporary christian musician from e\n",
            "  l dorado arkansas he is best known for the radio singles\n",
            "\n",
            "==========================================================================================\n",
            "Cluster 5  (346 docs)\n",
            "he:0.081 tour:0.051 his:0.048 world:0.046 at:0.045 \n",
            "\n",
            "* Jay Haas                                           0.91180\n",
            "  jay dean haas born december 2 1953 is an american professional golfer formerly of the pga \n",
            "  tour who now plays on the champions tourhaas was\n",
            "\n",
            "* Mark Hensby                                        0.91643\n",
            "  mark adam hensby born 29 june 1972 is an australian professional golfer he is known as the\n",
            "   forgotten man of australian golfhensby was born in\n",
            "\n",
            "* Andrew Buckle                                      0.91949\n",
            "  andrew nicholas buckle 24 september 1982 is an australian professional golferbuckle was bo\n",
            "  rn in brisbane queensland he had a promising amateur career including two wins\n",
            "\n",
            "* Gene Sauers                                        0.92007\n",
            "  gene craig sauers born august 22 1962 is an american professional golfer who has played on\n",
            "   the pga tour and currently plays on the champions\n",
            "\n",
            "* Mike Brisky                                        0.92243\n",
            "  michael charles brisky born may 28 1965 is an american professional golfer who played on t\n",
            "  he pga tour and the nationwide tourbrisky attended pan american\n",
            "\n",
            "==========================================================================================\n",
            "Cluster 6  (191 docs)\n",
            "art:0.151 museum:0.072 gallery:0.051 he:0.049 his:0.046 \n",
            "\n",
            "* Tom Moody (artist)                                 0.88092\n",
            "  tom moody is a visual artist critic and blogger based in new york city he began his career\n",
            "   as a painter using traditional materials but\n",
            "\n",
            "* Michael Hafftka                                    0.88116\n",
            "  michael hafftka is an american figurative expressionist painter living in new york city hi\n",
            "  s work is represented in the permanent collections of a number of\n",
            "\n",
            "* Burton Silverman                                   0.88720\n",
            "  burton silverman born 1928 is an american paintera 1949 graduate of columbia university si\n",
            "  lvermans work has concentrated on as he put it the landscape of\n",
            "\n",
            "* Charles Arnoldi                                    0.89113\n",
            "  charles arnoldi also known as chuck arnoldi and as charles arthur arnoldi is an american p\n",
            "  ainter sculptor and printmaker he was born april 10 1946\n",
            "\n",
            "* Steven Montgomery                                  0.89320\n",
            "  steven montgomery is an american artist born in detroit 1954 most often associated with la\n",
            "  rge scale ceramic sculpture suggesting industrial objects or mechanical detritus he\n",
            "\n",
            "==========================================================================================\n",
            "Cluster 7  (170 docs)\n",
            "music:0.124 orchestra:0.110 he:0.066 symphony:0.061 opera:0.059 \n",
            "\n",
            "* David Oei                                          0.88056\n",
            "  david oei chinese name pinyin hung jln surname pronounced wee is hong kongborn american cl\n",
            "  assical pianist b 1950 in hong kongoei was born in hong\n",
            "\n",
            "* Daniel Meyer (conductor)                           0.88816\n",
            "  daniel meyer was born in cleveland ohio and has been conductor and musical director of sev\n",
            "  eral prominent american orchestrashe is a graduate of denison university\n",
            "\n",
            "* Xian Zhang                                         0.89002\n",
            "  xian zhang chinese born in 1973 in dandong liaoning is a chinese american conductoras a ch\n",
            "  ild zhang began to learn music with her mother on\n",
            "\n",
            "* Stefano Miceli                                     0.89997\n",
            "  stefano miceli born 14 april 1975 is an italian classical pianist and conductorhe made his\n",
            "   concert debut at the age of 11 then he moved\n",
            "\n",
            "* Robert Creech                                      0.90146\n",
            "  robert edward creech born 26 september 1928 victoria british columbia is a canadian french\n",
            "   hornist music educator and arts administrator he served as director of\n",
            "\n",
            "==========================================================================================\n",
            "Cluster 8  (1231 docs)\n",
            "he:0.079 was:0.054 as:0.040 for:0.037 his:0.033 \n",
            "\n",
            "* Doug Naysmith                                      0.94268\n",
            "  john douglas naysmith born 1 april 1941 is a british labour cooperative politician who was\n",
            "   the member of parliament mp for bristol north west from\n",
            "\n",
            "* Bill Clinton                                       0.94311\n",
            "  william jefferson bill clinton born william jefferson blythe iii august 19 1946 is an amer\n",
            "  ican politician who served from 1993 to 2001 as the 42nd\n",
            "\n",
            "* John Garamendi                                     0.94778\n",
            "  john raymond garamendi born january 24 1945 is an american rancher businessman politician \n",
            "  and member of the democratic party who has represented areas of northern\n",
            "\n",
            "* Steve May                                          0.94804\n",
            "  steve may born c 1972 is a former politician from arizona where he served in the arizona h\n",
            "  ouse of representatives he was openly gay when\n",
            "\n",
            "* Levin H. Campbell                                  0.94902\n",
            "  levin hicks campbell born january 2 1927 is an american federal appellate judge on senior \n",
            "  status with the united states court of appeals for the\n",
            "\n",
            "==========================================================================================\n",
            "Cluster 9  (335 docs)\n",
            "film:0.120 he:0.060 for:0.047 television:0.046 series:0.042 \n",
            "\n",
            "* Robert Braiden                                     0.90570\n",
            "  robert braiden is an australian film director and writer born in sydney he grew up in moor\n",
            "  ebank liverpool new south wales and now currently lives\n",
            "\n",
            "* Paul Swadel                                        0.91027\n",
            "  paul swadel is a new zealand film director and producerhe has directed and produced many s\n",
            "  uccessful short films which have screened in competition at cannes\n",
            "\n",
            "* Bruce Redman                                       0.91450\n",
            "  dr bruce redman born 25 april 1960 is an australian film director film critic radio person\n",
            "  ality and media relations manager he currently works for the\n",
            "\n",
            "* Matthew Saville                                    0.91656\n",
            "  matthew saville is an australian television and film director who began his career working\n",
            "   as a titles designer for many australian television series several of\n",
            "\n",
            "* Hossein Shahabi                                    0.91684\n",
            "  hossein shahabi persian is an iranian film director screenwriter and film producer he belo\n",
            "  ngs is to the third generation of iranian new wave who was\n",
            "\n",
            "==========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFEmQgrkqEt9",
        "colab_type": "text"
      },
      "source": [
        "We no longer have the clear split between men and women. Cluters 0 and 2 appear to be still mixed, but others are quite consistent in content.\n",
        "* Cluster 0: notable women\n",
        "* Cluster 1: baseball players\n",
        "* Cluster 2: researchers, professors\n",
        "* Cluster 3: football(soccer)\n",
        "* Cluster 4: musicians, singers, song writers\n",
        "* Cluster 5: golfers\n",
        "* Cluster 6: painters, scultpers, artists\n",
        "* Cluster 7: orchestral musicians, conductors\n",
        "* Cluster 8: politicians, political personel\n",
        "* Cluster 9: film directors|\n",
        "\n",
        "Clusters are now more pure, but some are qualitatively \"bigger\" than others. For instance, the category of scholars is more general than the category of film directors. Increasing the number of clusters may split larger clusters. Another way to look at the size of cluster is to count the number of articles in each cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAiddEjHqEt-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ed85bfa-3fd8-422f-b5d3-768b8fd407ec"
      },
      "source": [
        "np.bincount(all_cluster_assignment[10])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 872,  435,  642,  399, 1286,  346,  191,  170, 1231,  335])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKRa8gtXqEuA",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    <h4>Question 4</h4> \n",
        "    <p>\n",
        "        Which of the 10 clusters above contains the <b>greatest</b> number of articles?\n",
        "    </p>\n",
        "    <p><i>Note: For this problem you don't need to write any code except you do need to run the code above to look at the clusters.</i></p>\n",
        "    <p>\n",
        "        <b>Gradescope:</b> Select one choice.\n",
        "    </p>\n",
        "    <ul>\n",
        "        <li>Cluster 0: notable women</li>\n",
        "        <li>Cluster 4: musicians, singers, song writers</li>\n",
        "        <li>Cluster 5: golfers</li>\n",
        "        <li>Cluster 7: orchestral musicians, conductors</li>\n",
        "        <li>Cluster 9: film directors</li>\n",
        "    </ul>\n",
        "</div>\n",
        "\n",
        "<div class=\"alert alert-block alert-success\">\n",
        "    <h4>Question 5</h4> \n",
        "    <p>\n",
        "        Which of the 10 clusters above contains the <b>least</b> number of articles?\n",
        "    </p>\n",
        "    <p><i>Note: For this problem you don't need to write any code except you do need to run the code above to look at the clusters.</i></p>\n",
        "    <p>\n",
        "        <b>Gradescope:</b> Select one choice.\n",
        "    </p>\n",
        "    <ul>\n",
        "        <li>Cluster 2: researchers, professors</li>\n",
        "        <li>Cluster 3: football (soccer)</li>\n",
        "        <li>Cluster 6: painters, sculpters, artists</li>\n",
        "        <li>Cluster 7: orchestral musicians, conductors</li>\n",
        "        <li>Cluster 8: politicians, political personel</li>\n",
        "    </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cSpQE1UqEuB",
        "colab_type": "text"
      },
      "source": [
        "There appears to be at least some connection between the topical consistency of a cluster and the number of its member data points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q4u9EhVqEuB",
        "colab_type": "text"
      },
      "source": [
        "Let us visualize the case for K=25. For the sake of brevity, we do not print the content of documents. It turns out that the top words with highest TF-IDF weights in each cluster are representative of the cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE6Ih-buqEuC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "141cd976-cbef-4d0f-f21c-150187af9639"
      },
      "source": [
        "k = 25\n",
        "visualize_document_clusters(wiki, tf_idf, all_centroids[k], all_cluster_assignment[k], k,\n",
        "                            words, display_docs=0) # turn off text for brevity"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "Cluster 0  (71 docs)\n",
            "he:0.072 was:0.067 labor:0.062 for:0.052 legislative:0.049 \n",
            "==========================================================================================\n",
            "Cluster 1  (89 docs)\n",
            "church:0.133 bishop:0.094 he:0.090 was:0.057 theology:0.054 \n",
            "==========================================================================================\n",
            "Cluster 2  (35 docs)\n",
            "danish:0.052 company:0.044 ceo:0.041 business:0.041 he:0.035 \n",
            "==========================================================================================\n",
            "Cluster 3  (171 docs)\n",
            "league:0.137 baseball:0.123 he:0.096 major:0.071 games:0.064 \n",
            "==========================================================================================\n",
            "Cluster 4  (112 docs)\n",
            "she:0.219 her:0.072 was:0.067 party:0.051 minister:0.048 \n",
            "==========================================================================================\n",
            "Cluster 5  (29 docs)\n",
            "psychology:0.206 research:0.080 psychological:0.067 university:0.055 he:0.047 \n",
            "==========================================================================================\n",
            "Cluster 6  (72 docs)\n",
            "medical:0.092 medicine:0.087 health:0.079 research:0.063 he:0.057 \n",
            "==========================================================================================\n",
            "Cluster 7  (188 docs)\n",
            "he:0.073 district:0.067 was:0.054 senate:0.046 republican:0.046 \n",
            "==========================================================================================\n",
            "Cluster 8  (377 docs)\n",
            "he:0.097 football:0.068 league:0.066 club:0.062 season:0.058 \n",
            "==========================================================================================\n",
            "Cluster 9  (52 docs)\n",
            "minister:0.212 he:0.089 prime:0.073 was:0.068 government:0.052 \n",
            "==========================================================================================\n",
            "Cluster 10  (46 docs)\n",
            "tour:0.317 pga:0.243 golf:0.125 he:0.117 his:0.083 \n",
            "==========================================================================================\n",
            "Cluster 11  (444 docs)\n",
            "he:0.079 coach:0.047 team:0.046 his:0.042 was:0.041 \n",
            "==========================================================================================\n",
            "Cluster 12  (68 docs)\n",
            "law:0.238 he:0.073 legal:0.063 court:0.051 school:0.049 \n",
            "==========================================================================================\n",
            "Cluster 13  (315 docs)\n",
            "university:0.082 he:0.082 research:0.066 professor:0.055 at:0.051 \n",
            "==========================================================================================\n",
            "Cluster 14  (314 docs)\n",
            "he:0.059 published:0.050 book:0.049 his:0.041 for:0.040 \n",
            "==========================================================================================\n",
            "Cluster 15  (169 docs)\n",
            "art:0.166 museum:0.079 gallery:0.056 he:0.048 work:0.045 \n",
            "==========================================================================================\n",
            "Cluster 16  (465 docs)\n",
            "film:0.080 he:0.069 for:0.049 as:0.041 on:0.041 \n",
            "==========================================================================================\n",
            "Cluster 17  (413 docs)\n",
            "album:0.076 band:0.070 he:0.052 with:0.048 music:0.046 \n",
            "==========================================================================================\n",
            "Cluster 18  (104 docs)\n",
            "he:0.086 served:0.056 as:0.053 air:0.052 was:0.048 \n",
            "==========================================================================================\n",
            "Cluster 19  (219 docs)\n",
            "music:0.125 orchestra:0.090 he:0.070 symphony:0.048 with:0.047 \n",
            "==========================================================================================\n",
            "Cluster 20  (709 docs)\n",
            "she:0.182 her:0.137 was:0.038 for:0.038 on:0.032 \n",
            "==========================================================================================\n",
            "Cluster 21  (929 docs)\n",
            "he:0.059 his:0.043 was:0.038 for:0.030 as:0.029 \n",
            "==========================================================================================\n",
            "Cluster 22  (259 docs)\n",
            "he:0.074 board:0.040 as:0.040 was:0.038 for:0.036 \n",
            "==========================================================================================\n",
            "Cluster 23  (184 docs)\n",
            "he:0.094 party:0.082 election:0.067 was:0.067 as:0.044 \n",
            "==========================================================================================\n",
            "Cluster 24  (73 docs)\n",
            "hockey:0.199 nhl:0.091 season:0.088 he:0.085 league:0.075 \n",
            "==========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zrrQ7yqqEuD",
        "colab_type": "text"
      },
      "source": [
        "Looking at the representative examples and top words, we classify each cluster as follows. Notice the bolded items, which indicate the appearance of a new theme.\n",
        "* Cluster 0: **British labor party**\n",
        "* Cluster 1: **Bishops**\n",
        "* Cluster 2: **danish CEOs**\n",
        "* Cluster 3: baseball\n",
        "* Cluster 4: politicials\n",
        "* Cluster 5: **psychology researchers**\n",
        "* Cluster 6: **medical researchers**\n",
        "* Cluster 7: **republican politicians**\n",
        "* Cluster 8: football(soccer)\n",
        "* Cluster 9: **prime ministers**\n",
        "* Cluster 10: golfers\n",
        "* Cluster 11: coaches\n",
        "* Cluster 12: **lawers**\n",
        "* Cluster 13: researchers, professors\n",
        "* Cluster 14: writers\n",
        "* Cluster 15: artists, museaum workers\n",
        "* Cluster 16: film directors\n",
        "* Cluster 17: musicians\n",
        "* Cluster 18: **airforce commanders**\n",
        "* Cluster 19: orchestral musicians\n",
        "* Cluster 20: *unclear*\n",
        "* Cluster 21: *unclear*\n",
        "* Cluster 22: *unclear*\n",
        "* Cluster 23: politicians\n",
        "* Cluster 24: **hockey players**\n",
        "\n",
        "Indeed, increasing K achieved the desired effect of breaking up large clusters.  Depending on the application, this may or may not be preferable to the K=10 analysis.\n",
        "\n",
        "Let's take it to the extreme and set K=100. We have a suspicion that this value is too large. Let us look at the top words from each cluster:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx-YopxTqEuE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "025b32d0-79db-4970-f564-ab17e389d221"
      },
      "source": [
        "k=100\n",
        "visualize_document_clusters(wiki, tf_idf, all_centroids[k], all_cluster_assignment[k], k,\n",
        "                            words, display_docs=0)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "Cluster 0  (116 docs)\n",
            "her:0.094 she:0.089 music:0.051 for:0.045 with:0.037 \n",
            "==========================================================================================\n",
            "Cluster 1  (87 docs)\n",
            "coach:0.227 head:0.084 he:0.078 football:0.058 season:0.051 \n",
            "==========================================================================================\n",
            "Cluster 2  (14 docs)\n",
            "director:0.065 he:0.063 mr:0.058 hooker:0.050 jindal:0.050 \n",
            "==========================================================================================\n",
            "Cluster 3  (67 docs)\n",
            "football:0.133 afl:0.122 australian:0.096 he:0.079 played:0.075 \n",
            "==========================================================================================\n",
            "Cluster 4  (175 docs)\n",
            "music:0.139 he:0.071 his:0.051 with:0.047 for:0.043 \n",
            "==========================================================================================\n",
            "Cluster 5  (102 docs)\n",
            "championships:0.103 world:0.072 metres:0.069 he:0.068 at:0.066 \n",
            "==========================================================================================\n",
            "Cluster 6  (9 docs)\n",
            "yoga:0.236 iskcon:0.089 he:0.065 new:0.063 swami:0.062 \n",
            "==========================================================================================\n",
            "Cluster 7  (33 docs)\n",
            "opera:0.317 she:0.086 at:0.049 with:0.048 la:0.047 \n",
            "==========================================================================================\n",
            "Cluster 8  (110 docs)\n",
            "he:0.081 chairman:0.069 board:0.058 was:0.047 president:0.044 \n",
            "==========================================================================================\n",
            "Cluster 9  (101 docs)\n",
            "she:0.132 her:0.077 series:0.076 miss:0.076 role:0.054 \n",
            "==========================================================================================\n",
            "Cluster 10  (42 docs)\n",
            "law:0.292 legal:0.072 he:0.072 school:0.056 university:0.050 \n",
            "==========================================================================================\n",
            "Cluster 11  (60 docs)\n",
            "japanese:0.067 he:0.063 his:0.046 war:0.042 has:0.035 \n",
            "==========================================================================================\n",
            "Cluster 12  (66 docs)\n",
            "law:0.101 court:0.090 he:0.075 district:0.072 judge:0.071 \n",
            "==========================================================================================\n",
            "Cluster 13  (50 docs)\n",
            "minister:0.114 he:0.068 finance:0.064 economics:0.062 was:0.048 \n",
            "==========================================================================================\n",
            "Cluster 14  (156 docs)\n",
            "she:0.208 her:0.061 was:0.051 for:0.048 as:0.042 \n",
            "==========================================================================================\n",
            "Cluster 15  (20 docs)\n",
            "football:0.196 bailey:0.085 he:0.052 for:0.048 was:0.048 \n",
            "==========================================================================================\n",
            "Cluster 16  (67 docs)\n",
            "senate:0.075 he:0.073 republican:0.073 house:0.067 district:0.067 \n",
            "==========================================================================================\n",
            "Cluster 17  (29 docs)\n",
            "sox:0.178 league:0.138 baseball:0.132 he:0.100 red:0.100 \n",
            "==========================================================================================\n",
            "Cluster 18  (23 docs)\n",
            "bush:0.080 president:0.073 service:0.071 he:0.058 corps:0.056 \n",
            "==========================================================================================\n",
            "Cluster 19  (18 docs)\n",
            "manitoba:0.282 provincial:0.116 election:0.108 winnipeg:0.089 was:0.087 \n",
            "==========================================================================================\n",
            "Cluster 20  (43 docs)\n",
            "he:0.083 was:0.070 election:0.052 as:0.043 party:0.042 \n",
            "==========================================================================================\n",
            "Cluster 21  (25 docs)\n",
            "canadian:0.084 curling:0.078 championships:0.071 ontario:0.068 rink:0.055 \n",
            "==========================================================================================\n",
            "Cluster 22  (65 docs)\n",
            "he:0.076 his:0.050 was:0.047 hall:0.035 at:0.035 \n",
            "==========================================================================================\n",
            "Cluster 23  (136 docs)\n",
            "band:0.143 he:0.062 album:0.058 with:0.052 bands:0.043 \n",
            "==========================================================================================\n",
            "Cluster 24  (438 docs)\n",
            "he:0.057 his:0.039 for:0.034 was:0.031 as:0.029 \n",
            "==========================================================================================\n",
            "Cluster 25  (161 docs)\n",
            "he:0.099 league:0.080 rugby:0.078 cup:0.069 for:0.065 \n",
            "==========================================================================================\n",
            "Cluster 26  (37 docs)\n",
            "comedy:0.124 show:0.079 he:0.073 standup:0.057 comedian:0.049 \n",
            "==========================================================================================\n",
            "Cluster 27  (27 docs)\n",
            "was:0.059 he:0.055 court:0.050 murder:0.041 that:0.039 \n",
            "==========================================================================================\n",
            "Cluster 28  (62 docs)\n",
            "church:0.172 bishop:0.120 he:0.088 lds:0.060 was:0.059 \n",
            "==========================================================================================\n",
            "Cluster 29  (61 docs)\n",
            "hockey:0.242 nhl:0.112 season:0.080 ice:0.079 he:0.075 \n",
            "==========================================================================================\n",
            "Cluster 30  (33 docs)\n",
            "health:0.197 he:0.077 medical:0.058 medicine:0.053 hospital:0.048 \n",
            "==========================================================================================\n",
            "Cluster 31  (77 docs)\n",
            "design:0.082 for:0.058 he:0.052 animation:0.046 his:0.040 \n",
            "==========================================================================================\n",
            "Cluster 32  (22 docs)\n",
            "army:0.135 singh:0.083 he:0.072 general:0.069 military:0.064 \n",
            "==========================================================================================\n",
            "Cluster 33  (44 docs)\n",
            "poetry:0.217 poems:0.112 poet:0.070 published:0.064 she:0.057 \n",
            "==========================================================================================\n",
            "Cluster 34  (31 docs)\n",
            "he:0.061 bank:0.055 dublin:0.050 was:0.045 irish:0.042 \n",
            "==========================================================================================\n",
            "Cluster 35  (48 docs)\n",
            "he:0.057 his:0.054 that:0.048 was:0.048 for:0.036 \n",
            "==========================================================================================\n",
            "Cluster 36  (13 docs)\n",
            "triathlon:0.113 olympics:0.072 he:0.059 world:0.057 competed:0.056 \n",
            "==========================================================================================\n",
            "Cluster 37  (97 docs)\n",
            "league:0.142 baseball:0.128 he:0.104 major:0.079 season:0.077 \n",
            "==========================================================================================\n",
            "Cluster 38  (27 docs)\n",
            "he:0.086 chef:0.072 his:0.069 leslie:0.046 was:0.036 \n",
            "==========================================================================================\n",
            "Cluster 39  (29 docs)\n",
            "chess:0.245 open:0.093 he:0.079 grandmaster:0.055 championship:0.050 \n",
            "==========================================================================================\n",
            "Cluster 40  (22 docs)\n",
            "billion:0.056 he:0.043 china:0.040 saudi:0.034 business:0.033 \n",
            "==========================================================================================\n",
            "Cluster 41  (3 docs)\n",
            "piatt:0.233 weather:0.190 fiber:0.101 tampa:0.100 them:0.095 \n",
            "==========================================================================================\n",
            "Cluster 42  (45 docs)\n",
            "orchestra:0.119 music:0.114 chamber:0.104 he:0.060 symphony:0.054 \n",
            "==========================================================================================\n",
            "Cluster 43  (30 docs)\n",
            "piano:0.209 music:0.073 orchestra:0.071 he:0.068 symphony:0.066 \n",
            "==========================================================================================\n",
            "Cluster 44  (15 docs)\n",
            "rabbi:0.217 jewish:0.089 he:0.068 green:0.066 religious:0.052 \n",
            "==========================================================================================\n",
            "Cluster 45  (37 docs)\n",
            "assembly:0.082 jersey:0.075 he:0.070 state:0.060 votes:0.054 \n",
            "==========================================================================================\n",
            "Cluster 46  (14 docs)\n",
            "software:0.081 he:0.076 his:0.046 computational:0.044 zend:0.043 \n",
            "==========================================================================================\n",
            "Cluster 47  (15 docs)\n",
            "scott:0.157 he:0.051 mcclure:0.049 pooh:0.048 2003s2003:0.044 \n",
            "==========================================================================================\n",
            "Cluster 48  (23 docs)\n",
            "guitar:0.134 duncan:0.059 he:0.053 with:0.043 band:0.040 \n",
            "==========================================================================================\n",
            "Cluster 49  (63 docs)\n",
            "physics:0.086 he:0.073 research:0.058 university:0.049 engineering:0.048 \n",
            "==========================================================================================\n",
            "Cluster 50  (89 docs)\n",
            "research:0.082 science:0.072 university:0.072 he:0.067 chemistry:0.052 \n",
            "==========================================================================================\n",
            "Cluster 51  (1 docs)\n",
            "todori:0.629 ivica:0.349 agrokor:0.279 croatian:0.150 zagreb:0.146 \n",
            "==========================================================================================\n",
            "Cluster 52  (252 docs)\n",
            "university:0.086 he:0.083 research:0.053 at:0.053 professor:0.050 \n",
            "==========================================================================================\n",
            "Cluster 53  (44 docs)\n",
            "nfl:0.156 yards:0.142 football:0.125 he:0.081 for:0.062 \n",
            "==========================================================================================\n",
            "Cluster 54  (61 docs)\n",
            "art:0.160 her:0.069 she:0.065 work:0.047 artist:0.042 \n",
            "==========================================================================================\n",
            "Cluster 55  (51 docs)\n",
            "governor:0.106 election:0.078 he:0.073 party:0.072 was:0.058 \n",
            "==========================================================================================\n",
            "Cluster 56  (6 docs)\n",
            "keith:0.190 acappella:0.122 willis:0.098 scharf:0.064 linux:0.060 \n",
            "==========================================================================================\n",
            "Cluster 57  (51 docs)\n",
            "theatre:0.220 play:0.050 directed:0.050 at:0.049 for:0.048 \n",
            "==========================================================================================\n",
            "Cluster 58  (7 docs)\n",
            "ski:0.344 popangelov:0.087 alhajuj:0.073 bulgarian:0.071 winter:0.071 \n",
            "==========================================================================================\n",
            "Cluster 59  (38 docs)\n",
            "he:0.084 for:0.046 his:0.046 league:0.045 baseball:0.043 \n",
            "==========================================================================================\n",
            "Cluster 60  (169 docs)\n",
            "he:0.102 was:0.062 as:0.050 from:0.042 for:0.036 \n",
            "==========================================================================================\n",
            "Cluster 61  (49 docs)\n",
            "jazz:0.225 he:0.069 with:0.062 his:0.047 music:0.046 \n",
            "==========================================================================================\n",
            "Cluster 62  (31 docs)\n",
            "campaign:0.096 he:0.050 for:0.044 was:0.043 as:0.040 \n",
            "==========================================================================================\n",
            "Cluster 63  (94 docs)\n",
            "art:0.182 museum:0.111 gallery:0.071 he:0.059 his:0.056 \n",
            "==========================================================================================\n",
            "Cluster 64  (13 docs)\n",
            "rt:0.138 irish:0.102 radio:0.086 he:0.068 bridgeman:0.061 \n",
            "==========================================================================================\n",
            "Cluster 65  (2 docs)\n",
            "ruiz:0.263 hjuler:0.201 baer:0.181 mama:0.176 kommissar:0.134 \n",
            "==========================================================================================\n",
            "Cluster 66  (45 docs)\n",
            "he:0.082 justice:0.066 was:0.047 turkish:0.040 as:0.039 \n",
            "==========================================================================================\n",
            "Cluster 67  (33 docs)\n",
            "orchestra:0.253 conductor:0.138 symphony:0.121 music:0.115 philharmonic:0.104 \n",
            "==========================================================================================\n",
            "Cluster 68  (35 docs)\n",
            "medicine:0.094 cancer:0.091 medical:0.088 research:0.081 clinical:0.068 \n",
            "==========================================================================================\n",
            "Cluster 69  (20 docs)\n",
            "buddhist:0.094 oxford:0.079 tibetan:0.071 he:0.062 buddhism:0.052 \n",
            "==========================================================================================\n",
            "Cluster 70  (5 docs)\n",
            "forsberg:0.085 tv:0.083 hearing:0.080 voice:0.076 reffett:0.076 \n",
            "==========================================================================================\n",
            "Cluster 71  (21 docs)\n",
            "mayor:0.081 councillor:0.079 committee:0.070 he:0.068 council:0.061 \n",
            "==========================================================================================\n",
            "Cluster 72  (20 docs)\n",
            "comic:0.114 comics:0.087 san:0.060 francisco:0.057 he:0.056 \n",
            "==========================================================================================\n",
            "Cluster 73  (10 docs)\n",
            "poker:0.535 wsop:0.179 event:0.153 limit:0.117 tournament:0.100 \n",
            "==========================================================================================\n",
            "Cluster 74  (64 docs)\n",
            "basketball:0.167 nba:0.105 points:0.085 he:0.080 player:0.064 \n",
            "==========================================================================================\n",
            "Cluster 75  (14 docs)\n",
            "she:0.262 baseball:0.208 her:0.155 girls:0.134 league:0.134 \n",
            "==========================================================================================\n",
            "Cluster 76  (9 docs)\n",
            "irving:0.083 publishing:0.075 bonnycastle:0.074 dick:0.066 nyholm:0.065 \n",
            "==========================================================================================\n",
            "Cluster 77  (64 docs)\n",
            "was:0.069 he:0.064 prison:0.051 on:0.044 charges:0.043 \n",
            "==========================================================================================\n",
            "Cluster 78  (154 docs)\n",
            "he:0.103 club:0.068 team:0.065 season:0.062 football:0.059 \n",
            "==========================================================================================\n",
            "Cluster 79  (30 docs)\n",
            "german:0.126 he:0.118 his:0.056 was:0.047 germany:0.045 \n",
            "==========================================================================================\n",
            "Cluster 80  (212 docs)\n",
            "she:0.238 her:0.141 at:0.041 was:0.040 with:0.035 \n",
            "==========================================================================================\n",
            "Cluster 81  (6 docs)\n",
            "hearst:0.125 leech:0.118 meter:0.107 hurdles:0.090 martel:0.088 \n",
            "==========================================================================================\n",
            "Cluster 82  (127 docs)\n",
            "her:0.217 she:0.141 was:0.041 for:0.037 on:0.031 \n",
            "==========================================================================================\n",
            "Cluster 83  (64 docs)\n",
            "racing:0.135 championship:0.083 he:0.082 race:0.074 car:0.074 \n",
            "==========================================================================================\n",
            "Cluster 84  (150 docs)\n",
            "film:0.202 films:0.060 he:0.053 festival:0.049 for:0.044 \n",
            "==========================================================================================\n",
            "Cluster 85  (27 docs)\n",
            "voice:0.088 series:0.070 he:0.063 television:0.051 actor:0.047 \n",
            "==========================================================================================\n",
            "Cluster 86  (78 docs)\n",
            "minister:0.135 party:0.091 election:0.077 liberal:0.075 was:0.071 \n",
            "==========================================================================================\n",
            "Cluster 87  (11 docs)\n",
            "he:0.067 guest:0.060 elevator:0.056 film:0.054 television:0.053 \n",
            "==========================================================================================\n",
            "Cluster 88  (30 docs)\n",
            "wrestling:0.082 hop:0.073 hip:0.072 he:0.063 wwe:0.058 \n",
            "==========================================================================================\n",
            "Cluster 89  (27 docs)\n",
            "song:0.214 eurovision:0.096 contest:0.063 she:0.048 album:0.041 \n",
            "==========================================================================================\n",
            "Cluster 90  (148 docs)\n",
            "album:0.143 released:0.068 her:0.051 music:0.051 on:0.047 \n",
            "==========================================================================================\n",
            "Cluster 91  (25 docs)\n",
            "air:0.219 force:0.106 commander:0.090 command:0.087 naval:0.075 \n",
            "==========================================================================================\n",
            "Cluster 92  (140 docs)\n",
            "novel:0.073 published:0.064 book:0.059 fiction:0.045 her:0.041 \n",
            "==========================================================================================\n",
            "Cluster 93  (100 docs)\n",
            "he:0.087 news:0.086 for:0.058 on:0.046 new:0.044 \n",
            "==========================================================================================\n",
            "Cluster 94  (18 docs)\n",
            "parish:0.145 he:0.055 was:0.045 his:0.039 greenberg:0.038 \n",
            "==========================================================================================\n",
            "Cluster 95  (22 docs)\n",
            "league:0.142 runs:0.116 baseball:0.093 yankees:0.089 he:0.081 \n",
            "==========================================================================================\n",
            "Cluster 96  (86 docs)\n",
            "radio:0.123 show:0.088 he:0.076 on:0.062 his:0.045 \n",
            "==========================================================================================\n",
            "Cluster 97  (14 docs)\n",
            "rao:0.113 film:0.087 he:0.074 indian:0.072 telugu:0.071 \n",
            "==========================================================================================\n",
            "Cluster 98  (43 docs)\n",
            "tour:0.325 pga:0.258 golf:0.134 he:0.116 his:0.086 \n",
            "==========================================================================================\n",
            "Cluster 99  (9 docs)\n",
            "theory:0.099 relativity:0.085 that:0.075 ortiz:0.065 he:0.062 \n",
            "==========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZgU3lAXqEuF",
        "colab_type": "text"
      },
      "source": [
        "**A high value of K encourages pure clusters, but we cannot keep increasing K. For large enough K, related documents end up going to different clusters.**\n",
        "\n",
        "That said, the result for K=100 is not entirely bad. After all, it gives us separate clusters for such categories as Scotland, Brazil, LGBT, computer science and the Mormon Church. If we set K somewhere between 25 and 100, we should be able to avoid breaking up clusters while discovering new ones.\n",
        "\n",
        "Also, we should ask ourselves how much **granularity** we want in our clustering. If we wanted a rough sketch of Wikipedia, we don't want too detailed clusters. On the other hand, having many clusters can be valuable when we are zooming into a certain part of Wikipedia.\n",
        "\n",
        "**There is no golden rule for choosing K. It all depends on the particular application and domain we are in.**\n",
        "\n",
        "Another heuristic people use that does not rely on so much visualization, which can be hard in many applications (including here!) is as follows.  Track heterogeneity versus K and look for the \"elbow\" of the curve where the heterogeneity decrease rapidly before this value of K, but then only gradually for larger values of K.  This naturally trades off between trying to minimize heterogeneity, but reduce model complexity.  In the heterogeneity versus K plot made above, we did not yet really see a flattening out of the heterogeneity, which might indicate that indeed K=100 is \"reasonable\" and we only see real overfitting for larger values of K (which are even harder to visualize using the methods we attempted above.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrVvrzP9qEuG",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    <h4>Question 6</h4> \n",
        "    <p>\n",
        "        Another sign of too large K is having lots of small clusters. Look at the distribution of cluster sizes (by number of member data points). When doing k-means with k=100, how many of the clusters have fewer than 44 articles (i.e. 0.004% of the dataset)?\n",
        "    </p>\n",
        "    <p>\n",
        "        <b>Gradescope:</b> Type in your number as an integer (e.g. 42).\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAJJVhd0qEuG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5241ec9a-4eb5-4d94-8d15-2f42e5788a1a"
      },
      "source": [
        "# TODO\n",
        "counts = np.bincount(all_cluster_assignment[100])\n",
        "len(counts[counts<44]) #yes "
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi6cX4BJqEuH",
        "colab_type": "text"
      },
      "source": [
        "Keep in mind though that tiny clusters aren't necessarily bad. A tiny cluster of documents that really look like each others is definitely preferable to a medium-sized cluster of documents with mixed content. However, having too few articles in a cluster may lead us to question if that cluster is really worth separating from the others."
      ]
    }
  ]
}